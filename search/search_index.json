{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Apache Airflow Este reposit\u00f3rio cont\u00e9m um conjunto de arquivos markdown que explicam de forma bem b\u00e1sica, como usar o Apache Airflow. O curso \u00e9 voltado para o tipo de pessoa que nunca teve o contato com o software Apache Airflow ou qualquer outro software baseado em DAGs. Este material foi produzido como parte de um projeto em Inicia\u00e7\u00e3o cient\u00edfica da Universidade Federal do Vale do S\u00e3o Francisco. Para ter acesso ao v\u00eddeos, materiais em PDF e resolu\u00e7\u00e3o dos exerc\u00edcios propostos, inscreva-se em nosso curso gratuito na Udemy . Para ter somente acesso aos v\u00eddeos, assista nossa playlist no Youtube .","title":"In\u00edcio"},{"location":"#apache-airflow","text":"Este reposit\u00f3rio cont\u00e9m um conjunto de arquivos markdown que explicam de forma bem b\u00e1sica, como usar o Apache Airflow. O curso \u00e9 voltado para o tipo de pessoa que nunca teve o contato com o software Apache Airflow ou qualquer outro software baseado em DAGs. Este material foi produzido como parte de um projeto em Inicia\u00e7\u00e3o cient\u00edfica da Universidade Federal do Vale do S\u00e3o Francisco. Para ter acesso ao v\u00eddeos, materiais em PDF e resolu\u00e7\u00e3o dos exerc\u00edcios propostos, inscreva-se em nosso curso gratuito na Udemy . Para ter somente acesso aos v\u00eddeos, assista nossa playlist no Youtube .","title":"Apache Airflow"},{"location":"about/","text":"Sobre o Autor Meu nome \u00e9 Daniel Alencar! Tenho 20 anos e estudo engenharia da Computa\u00e7\u00e3o na Universidade Federal do vale do S\u00e3o Francisco. Entre em contato!","title":"Sobre o Autor"},{"location":"about/#sobre-o-autor","text":"Meu nome \u00e9 Daniel Alencar! Tenho 20 anos e estudo engenharia da Computa\u00e7\u00e3o na Universidade Federal do vale do S\u00e3o Francisco. Entre em contato!","title":"Sobre o Autor"},{"location":"project/","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"project/#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"project/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"project/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"1%20-%20Introdu%C3%A7%C3%A3o%20ao%20Apache%20Airflow/Exerc%C3%ADcio%2001/","text":"Exerc\u00edcio 01 Como exerc\u00edcio, n\u00f3s sugerimos que voc\u00ea explore a ferramenta: Execute outras DAGs Feche o programa no terminal e o reabra novamente Verifique se ocorreram erros Clique nos bot\u00f5es da interface","title":"Exerc\u00edcio 01"},{"location":"1%20-%20Introdu%C3%A7%C3%A3o%20ao%20Apache%20Airflow/Exerc%C3%ADcio%2001/#exercicio-01","text":"Como exerc\u00edcio, n\u00f3s sugerimos que voc\u00ea explore a ferramenta: Execute outras DAGs Feche o programa no terminal e o reabra novamente Verifique se ocorreram erros Clique nos bot\u00f5es da interface","title":"Exerc\u00edcio 01"},{"location":"1%20-%20Introdu%C3%A7%C3%A3o%20ao%20Apache%20Airflow/Preparac%CC%A7a%CC%83o%20do%20Airflow/","text":"Prepara\u00e7\u00e3o do Airflow Com o nosso ambiente preparado, estamos prontos para a instala\u00e7\u00e3o efetiva do Airflow. \u26a0\ufe0f Ao copiar os comandos, talvez haja alguma quebra de linha no texto. Isto acontece no material em PDF. Cria\u00e7\u00e3o do projeto Define o diret\u00f3rio padr\u00e3o do Airflow: export AIRFLOW_HOME=~/airflow Determine a vers\u00e3o do Airflow que instalaremos: AIRFLOW_VERSION=2.0.1 Determine a vers\u00e3o do Python utilizada pela sua m\u00e1quina: PYTHON_VERSION=\"$(python --version | cut -d \" \" -f 2 | cut -d \".\" -f 1-2)\" Define uma vari\u00e1vel que \u00e9 um link que cont\u00e9m todas as depend\u00eancias do Airflow de acordo com a vers\u00e3o do airflow escolhida e a vers\u00e3o do python3 da sua m\u00e1quina: CONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\" Instala\u00e7\u00e3o final do Airflow: pip3 install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\" Configura\u00e7\u00f5es do projeto Inicializa o banco de dados padr\u00e3o: airflow db init Se por acaso, for obtido o erro AttributeError: module 'wtforms.fields' has no attribute 'TextField' , podemos instalar a biblioteca flask-ldap-login em sua vers\u00e3o 0.3.0. Isto resolver\u00e1 o problema. Ao que parece, esta biblioteca usa uma vers\u00e3o mais antiga da biblioteca wtforms que ainda continha o atributo \u2018TextField\u2019. Pode ser que simplesmente instalando a biblioteca wtforms nessa vers\u00e3o especifica resolva o problema. A vers\u00e3o 0.8.2 da wtforms ainda continha o atributo \u2018TextField\u2019. Corre\u00e7\u00e3o do erro Execute os seguintes comandos no seu terminal (dentro da pasta do airflow) caso tenha enfrentado o erro dito acima. pip3 install flask-ldap-login==0.3.0 deactivate source NOME/bin/activate Dentro da pasta definida por AIRFLOW_HOME , apague: logs airflow.cfg airflow.db webserver_config.py para que novos arquivos venham ser criados a partir do comando abaixo: airflow db init Com isso, o problema n\u00e3o deve aparecer mais. Cria\u00e7\u00e3o de usu\u00e1rio Crie um usu\u00e1rio admin substituindo os par\u00e2metros USERNAME, FIRSTNAME, LASTNAME e EMAIL. airflow users create --username USERNAME --firstname FIRSTNAME --lastname LASTNAME --role Admin --email EMAIL Ap\u00f3s isto, defina a senha de autentica\u00e7\u00e3o que vai ser pedida no terminal. Inicializando interface Aqui utilizaremos a porta 8080, por\u00e9m, caso n\u00e3o seja poss\u00edvel utilizar esta porta, pode-se alterar o valor tamb\u00e9m. Este \u00e9 o comando que inicializa o Airflow. airflow webserver -p 8080 Deixe o comando acima rodando e abra outro terminal na mesma pasta \u2018airflow-test\u2019. Neste terminal, habilite tamb\u00e9m o ambiente virtual criado anteriormente com o seguinte comando: source NOME/bin/activate Ap\u00f3s isto, digite o comando: airflow scheduler Utilizando a interface Inicializando http://localhost:8080/ no navegador, teremos a seguinte tela: Entre com o seu usu\u00e1rio e senha. E assim, a pr\u00f3xima tela de exibi\u00e7\u00e3o ser\u00e1 a seguinte. E pronto, temos o Airflow totalmente configurado para ser executado em nosso sistema. Como exemplo, podemos executar um destes arquivos da lista. Procure o exemplo \u2018tutorial\u2019 e clique nele: Ap\u00f3s isto, clique no bot\u00e3o superior esquerdo para executar. E pronto, j\u00e1 estamos executando uma DAG em nosso airflow. No pr\u00f3ximo artigo entenderemos melhor a ideia de DAGs. Terminando a execu\u00e7\u00e3o Para fecharmos o airflow em nosso sistema, devemos parar as execu\u00e7\u00f5es do airflow webserver -p 8080 e airflow scheduler em nossos terminais. Para fazer isto, podemos clicar CTRL + C nos dois terminais. E assim, o airflow fecha a sua execu\u00e7\u00e3o. Adendo (talvez n\u00e3o precise ser feito) Na pr\u00f3xima vez que for rodar o airflow com o comando airflow webserver -p 8080 e airflow scheduler pode ser que ocorra o seguinte erro: sqlite3.OperationalError: no such table: dag Isso aconteceu porque as tabelas ab_* n\u00e3o foram criadas no airflow db init . Todas essas tabelas s\u00e3o para controle de acesso baseado em fun\u00e7\u00e3o \u2013 RBAC. Para resolver este problema, edite o arquivo airflow.cfg colocando/modificando a seguinte linha de c\u00f3digo. [webserver] rbac = True Digite novamente: airflow db init E ap\u00f3s isto, recrie o usu\u00e1rio: airflow users create --username USERNAME --firstname FIRSTNAME --lastname LASTNAME --role Admin --email EMAIL E rode o airflow novamente: airflow webserver -p 8080 airflow scheduler Com isto, voc\u00ea j\u00e1 est\u00e1 apto a utilizar o airflow sempre que quiser.","title":"Prepara\u00e7\u00e3o do Airflow"},{"location":"1%20-%20Introdu%C3%A7%C3%A3o%20ao%20Apache%20Airflow/Preparac%CC%A7a%CC%83o%20do%20Airflow/#preparacao-do-airflow","text":"Com o nosso ambiente preparado, estamos prontos para a instala\u00e7\u00e3o efetiva do Airflow. \u26a0\ufe0f Ao copiar os comandos, talvez haja alguma quebra de linha no texto. Isto acontece no material em PDF.","title":"Prepara\u00e7\u00e3o do Airflow"},{"location":"1%20-%20Introdu%C3%A7%C3%A3o%20ao%20Apache%20Airflow/Preparac%CC%A7a%CC%83o%20do%20Airflow/#criacao-do-projeto","text":"Define o diret\u00f3rio padr\u00e3o do Airflow: export AIRFLOW_HOME=~/airflow Determine a vers\u00e3o do Airflow que instalaremos: AIRFLOW_VERSION=2.0.1 Determine a vers\u00e3o do Python utilizada pela sua m\u00e1quina: PYTHON_VERSION=\"$(python --version | cut -d \" \" -f 2 | cut -d \".\" -f 1-2)\" Define uma vari\u00e1vel que \u00e9 um link que cont\u00e9m todas as depend\u00eancias do Airflow de acordo com a vers\u00e3o do airflow escolhida e a vers\u00e3o do python3 da sua m\u00e1quina: CONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\" Instala\u00e7\u00e3o final do Airflow: pip3 install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"","title":"Cria\u00e7\u00e3o do projeto"},{"location":"1%20-%20Introdu%C3%A7%C3%A3o%20ao%20Apache%20Airflow/Preparac%CC%A7a%CC%83o%20do%20Airflow/#configuracoes-do-projeto","text":"Inicializa o banco de dados padr\u00e3o: airflow db init Se por acaso, for obtido o erro AttributeError: module 'wtforms.fields' has no attribute 'TextField' , podemos instalar a biblioteca flask-ldap-login em sua vers\u00e3o 0.3.0. Isto resolver\u00e1 o problema. Ao que parece, esta biblioteca usa uma vers\u00e3o mais antiga da biblioteca wtforms que ainda continha o atributo \u2018TextField\u2019. Pode ser que simplesmente instalando a biblioteca wtforms nessa vers\u00e3o especifica resolva o problema. A vers\u00e3o 0.8.2 da wtforms ainda continha o atributo \u2018TextField\u2019.","title":"Configura\u00e7\u00f5es do projeto"},{"location":"1%20-%20Introdu%C3%A7%C3%A3o%20ao%20Apache%20Airflow/Preparac%CC%A7a%CC%83o%20do%20Airflow/#correcao-do-erro","text":"Execute os seguintes comandos no seu terminal (dentro da pasta do airflow) caso tenha enfrentado o erro dito acima. pip3 install flask-ldap-login==0.3.0 deactivate source NOME/bin/activate Dentro da pasta definida por AIRFLOW_HOME , apague: logs airflow.cfg airflow.db webserver_config.py para que novos arquivos venham ser criados a partir do comando abaixo: airflow db init Com isso, o problema n\u00e3o deve aparecer mais.","title":"Corre\u00e7\u00e3o do erro"},{"location":"1%20-%20Introdu%C3%A7%C3%A3o%20ao%20Apache%20Airflow/Preparac%CC%A7a%CC%83o%20do%20Airflow/#criacao-de-usuario","text":"Crie um usu\u00e1rio admin substituindo os par\u00e2metros USERNAME, FIRSTNAME, LASTNAME e EMAIL. airflow users create --username USERNAME --firstname FIRSTNAME --lastname LASTNAME --role Admin --email EMAIL Ap\u00f3s isto, defina a senha de autentica\u00e7\u00e3o que vai ser pedida no terminal.","title":"Cria\u00e7\u00e3o de usu\u00e1rio"},{"location":"1%20-%20Introdu%C3%A7%C3%A3o%20ao%20Apache%20Airflow/Preparac%CC%A7a%CC%83o%20do%20Airflow/#inicializando-interface","text":"Aqui utilizaremos a porta 8080, por\u00e9m, caso n\u00e3o seja poss\u00edvel utilizar esta porta, pode-se alterar o valor tamb\u00e9m. Este \u00e9 o comando que inicializa o Airflow. airflow webserver -p 8080 Deixe o comando acima rodando e abra outro terminal na mesma pasta \u2018airflow-test\u2019. Neste terminal, habilite tamb\u00e9m o ambiente virtual criado anteriormente com o seguinte comando: source NOME/bin/activate Ap\u00f3s isto, digite o comando: airflow scheduler","title":"Inicializando interface"},{"location":"1%20-%20Introdu%C3%A7%C3%A3o%20ao%20Apache%20Airflow/Preparac%CC%A7a%CC%83o%20do%20Airflow/#utilizando-a-interface","text":"Inicializando http://localhost:8080/ no navegador, teremos a seguinte tela: Entre com o seu usu\u00e1rio e senha. E assim, a pr\u00f3xima tela de exibi\u00e7\u00e3o ser\u00e1 a seguinte. E pronto, temos o Airflow totalmente configurado para ser executado em nosso sistema. Como exemplo, podemos executar um destes arquivos da lista. Procure o exemplo \u2018tutorial\u2019 e clique nele: Ap\u00f3s isto, clique no bot\u00e3o superior esquerdo para executar. E pronto, j\u00e1 estamos executando uma DAG em nosso airflow. No pr\u00f3ximo artigo entenderemos melhor a ideia de DAGs.","title":"Utilizando a interface"},{"location":"1%20-%20Introdu%C3%A7%C3%A3o%20ao%20Apache%20Airflow/Preparac%CC%A7a%CC%83o%20do%20Airflow/#terminando-a-execucao","text":"Para fecharmos o airflow em nosso sistema, devemos parar as execu\u00e7\u00f5es do airflow webserver -p 8080 e airflow scheduler em nossos terminais. Para fazer isto, podemos clicar CTRL + C nos dois terminais. E assim, o airflow fecha a sua execu\u00e7\u00e3o.","title":"Terminando a execu\u00e7\u00e3o"},{"location":"1%20-%20Introdu%C3%A7%C3%A3o%20ao%20Apache%20Airflow/Preparac%CC%A7a%CC%83o%20do%20Airflow/#adendo-talvez-nao-precise-ser-feito","text":"Na pr\u00f3xima vez que for rodar o airflow com o comando airflow webserver -p 8080 e airflow scheduler pode ser que ocorra o seguinte erro: sqlite3.OperationalError: no such table: dag Isso aconteceu porque as tabelas ab_* n\u00e3o foram criadas no airflow db init . Todas essas tabelas s\u00e3o para controle de acesso baseado em fun\u00e7\u00e3o \u2013 RBAC. Para resolver este problema, edite o arquivo airflow.cfg colocando/modificando a seguinte linha de c\u00f3digo. [webserver] rbac = True Digite novamente: airflow db init E ap\u00f3s isto, recrie o usu\u00e1rio: airflow users create --username USERNAME --firstname FIRSTNAME --lastname LASTNAME --role Admin --email EMAIL E rode o airflow novamente: airflow webserver -p 8080 airflow scheduler Com isto, voc\u00ea j\u00e1 est\u00e1 apto a utilizar o airflow sempre que quiser.","title":"Adendo (talvez n\u00e3o precise ser feito)"},{"location":"1%20-%20Introdu%C3%A7%C3%A3o%20ao%20Apache%20Airflow/Preparac%CC%A7a%CC%83o%20do%20projeto/","text":"Prepara\u00e7\u00e3o do projeto Agora que j\u00e1 sabemos um pouco sobre os problemas que o software Apache Airflow consegue resolver, podemos trabalhar diretamente com ele. Para fazer isto, podemos instal\u00e1-lo localmente em nosso computador. Esta e a pr\u00f3xima aula tem a inten\u00e7\u00e3o de realizar este prop\u00f3sito. \u26a0\ufe0f A instala\u00e7\u00e3o do Airflow foi feita em um Linux Mint 20.2 Cinnamon. Por\u00e9m, a instala\u00e7\u00e3o deve ocorrer normalmente (ou com poucas mudan\u00e7as) tamb\u00e9m em outros sistemas baseados no Ubuntu. Passos iniciais Antes de tudo, temos que ter instalado o python3 em nosso computador. Logo, execute o seguinte comando no terminal: sudo apt-get install python3 Para verificar isto, podemos colocar o seguinte comando no terminal: python3 --version Ap\u00f3s isto, precisamos do 'pip3' instalado (gerenciador de pacotes do python). Para isto, execute: sudo apt-get install python3-pip E em seguida, para verificar se deu tudo certo: pip3 --version Para o nosso projeto, n\u00e3o iremos trabalhar com o python global do sistema, para isto, precisamos de uma \u00e1rea espec\u00edfica com o python separado do python do nosso sistema. Iremos fazer isto atrav\u00e9s de uma biblioteca de cria\u00e7\u00e3o de ambientes virtuais python, uma delas \u00e9 a \u2018virtualenv\u2019. Para instal\u00e1-la devemos executar o seguinte comando: sudo pip3 install virtualenv E, mais uma vez, devemos verificar a instala\u00e7\u00e3o: virtualenv --version Prepara\u00e7\u00e3o do Projeto Criando a pasta do projeto e entrando nela no terminal: mkdir airflow-test cd airflow-test O Airflow dever\u00e1 ser instalado dentro de um ambiente virtual. Para criar um ambiente virtual, digite o seguinte comando (lembre-se de substituir o par\u00e2metro NOME): virtualenv NOME Agora, podemos ativar a virtualenv que acabamos de criar, para isto execute: source NOME/bin/activate","title":"Prepara\u00e7\u00e3o do projeto"},{"location":"1%20-%20Introdu%C3%A7%C3%A3o%20ao%20Apache%20Airflow/Preparac%CC%A7a%CC%83o%20do%20projeto/#preparacao-do-projeto","text":"Agora que j\u00e1 sabemos um pouco sobre os problemas que o software Apache Airflow consegue resolver, podemos trabalhar diretamente com ele. Para fazer isto, podemos instal\u00e1-lo localmente em nosso computador. Esta e a pr\u00f3xima aula tem a inten\u00e7\u00e3o de realizar este prop\u00f3sito. \u26a0\ufe0f A instala\u00e7\u00e3o do Airflow foi feita em um Linux Mint 20.2 Cinnamon. Por\u00e9m, a instala\u00e7\u00e3o deve ocorrer normalmente (ou com poucas mudan\u00e7as) tamb\u00e9m em outros sistemas baseados no Ubuntu.","title":"Prepara\u00e7\u00e3o do projeto"},{"location":"1%20-%20Introdu%C3%A7%C3%A3o%20ao%20Apache%20Airflow/Preparac%CC%A7a%CC%83o%20do%20projeto/#passos-iniciais","text":"Antes de tudo, temos que ter instalado o python3 em nosso computador. Logo, execute o seguinte comando no terminal: sudo apt-get install python3 Para verificar isto, podemos colocar o seguinte comando no terminal: python3 --version Ap\u00f3s isto, precisamos do 'pip3' instalado (gerenciador de pacotes do python). Para isto, execute: sudo apt-get install python3-pip E em seguida, para verificar se deu tudo certo: pip3 --version Para o nosso projeto, n\u00e3o iremos trabalhar com o python global do sistema, para isto, precisamos de uma \u00e1rea espec\u00edfica com o python separado do python do nosso sistema. Iremos fazer isto atrav\u00e9s de uma biblioteca de cria\u00e7\u00e3o de ambientes virtuais python, uma delas \u00e9 a \u2018virtualenv\u2019. Para instal\u00e1-la devemos executar o seguinte comando: sudo pip3 install virtualenv E, mais uma vez, devemos verificar a instala\u00e7\u00e3o: virtualenv --version","title":"Passos iniciais"},{"location":"1%20-%20Introdu%C3%A7%C3%A3o%20ao%20Apache%20Airflow/Preparac%CC%A7a%CC%83o%20do%20projeto/#preparacao-do-projeto_1","text":"Criando a pasta do projeto e entrando nela no terminal: mkdir airflow-test cd airflow-test O Airflow dever\u00e1 ser instalado dentro de um ambiente virtual. Para criar um ambiente virtual, digite o seguinte comando (lembre-se de substituir o par\u00e2metro NOME): virtualenv NOME Agora, podemos ativar a virtualenv que acabamos de criar, para isto execute: source NOME/bin/activate","title":"Prepara\u00e7\u00e3o do Projeto"},{"location":"1%20-%20Introdu%C3%A7%C3%A3o%20ao%20Apache%20Airflow/Sobre%20o%20Apache%20Airflow/","text":"Sobre o Apache Airflow Neste artigo, voc\u00ea ir\u00e1 conhecer um pouco sobre o que \u00e9 e do que se trata a ferramenta que permite automatizar a execu\u00e7\u00e3o de tarefas agendadas ( scheduled tasks ) de maneira inteligente e baseada em grafos ac\u00edclicos dirigidos ( Directed Acyclic Graph, DAG ). O que \u00e9 o Airflow ? O Airflow \u00e9 um software criado pelo Airbnb e escrito em Python. Se tornou um software de c\u00f3digo aberto em 2015 e logo depois foi cedida para a Apache Foundation. Seu objetivo \u00e9 orquestrar pipelines de tarefas agendadas por meio de arquivos python com instru\u00e7\u00f5es de sequenciamento definidas, chamados DAGs. Pense nele como um vers\u00e1til maestro, capaz de orquestrar diferentes m\u00fasicas, de diversos tempos e com diferentes instrumentos de maneira igualmente \u00f3tima. Para seu funcionamento, o Airflow conta com alguns elementos chave que permitem a exist\u00eancia da sinergia necess\u00e1ria entre tarefas, eventos, estados e filas, todos funcionando de maneira sincronizada e de acordo com configura\u00e7\u00f5es definidas pelo usu\u00e1rio. A Figura abaixo representa, de maneira relativamente simplificada e em uma mesma m\u00e1quina (visto que \u00e9 poss\u00edvel configurar o Airflow de maneira escalon\u00e1vel) a estrutura de uma inst\u00e2ncia da ferramenta: Sendo assim, descrevendo de maneira tamb\u00e9m simplificada a funcionalidade e comportamento de cada elemento apresentado na Figura anterior: Airflow.cfg: Arquivo de configura\u00e7\u00f5es que descreve, principalmente, as conex\u00f5es utilizadas para comunica\u00e7\u00e3o com o banco de dados de metadados da ferramenta, os intervalos de verifica\u00e7\u00e3o de novos arquivos DAGs, e a frequ\u00eancia de atualiza\u00e7\u00e3o dos estados correntes de cada tarefa. Web Server: Sub-sistema respons\u00e1vel pela integra\u00e7\u00e3o e execu\u00e7\u00e3o de uma interface visual para o usu\u00e1rio. Aqui s\u00e3o apresentados graficamente a maior parte dos elementos que podem ser utilizados pelo usu\u00e1rio, como DAGs, logs, alertas, avisos e todo tipo de monitoramento do sistema. Scheduler: Este componente pode ser entendido como o \u201ccora\u00e7\u00e3o\u201d do Airflow. No mundo musical, \u00e9 poss\u00edvel comparar o Scheduler a um metr\u00f4nomo, ou ao compasso que d\u00e1 o tempo \u00e0 m\u00fasica. Aqui, ele \u00e9 respons\u00e1vel pela temporiza\u00e7\u00e3o do sistema, resultando na execu\u00e7\u00e3o programada de DAGs, no agendamento de execu\u00e7\u00e3o de tarefas individuais das DAGs e tamb\u00e9m da distribui\u00e7\u00e3o destas para diferentes Executors. Resumidamente, garantir o bom funcionamento deste componente faz parte de um grande diferencial para garantir o bom funcionamento do Airflow como ferramenta, j\u00e1 que faz a integra\u00e7\u00e3o de quase todos os outros componentes/sub-sistemas. Metadata: Se o Scheduler pode ser considerado o \u201ccora\u00e7\u00e3o\u201d do Airflow, ent\u00e3o o banco de dados de metadados seria o \u201cc\u00e9rebro\u201d. \u00c9 neste elemento que s\u00e3o armazenadas todas as vari\u00e1veis utilizadas por todos os outros componentes da ferramenta, desde usu\u00e1rios at\u00e9 retornos de tarefas. Faz uso de um banco de dados relacional que permite, inclusive, a troca de informa\u00e7\u00e3o entre tarefas, principal causa de problemas de m\u00e1 utiliza\u00e7\u00e3o do Airflow, que ser\u00e3o discutidos no pr\u00f3ximo cap\u00edtulo. Executors: Sub-sistema respons\u00e1vel pela execu\u00e7\u00e3o das tarefas programadas pelo Scheduler, que est\u00e3o localizadas na fila (queue) deste sub-sistema (na atual representa\u00e7\u00e3o). O Airflow permite a execu\u00e7\u00e3o de diferentes tipos de tarefas atrav\u00e9s de operadores de diferentes naturezas, como o PythonOperator, para execu\u00e7\u00e3o de scripts Python, o DockerOperator, para trabalho com containers do Docker ou at\u00e9 mesmo o BashOperator, para a execu\u00e7\u00e3o de comandos bash. Essa versatilidade permite que cada tarefa possa ser executada isoladamente dentro do ambiente espec\u00edfico definido como Worker. Todas essas fun\u00e7\u00f5es s\u00e3o definidas (para a arquitetura local apresentada) e executadas dentro de um Executor, que ao fim comunica com o banco de metadados para informar o retorno das a\u00e7\u00f5es. Com a integra\u00e7\u00e3o de todos esses componentes, o usu\u00e1rio \u00e9 capaz ent\u00e3o de escrever e programar a execu\u00e7\u00e3o de diferentes conjuntos de tarefas ac\u00edclicas com uma imensa variedade de possibilidades para a execu\u00e7\u00e3o de cada tarefa, que v\u00e3o desde interpretadores Python, containers Docker e at\u00e9 mesmo comandos bash. O que o Airflow N\u00c3O \u00c9 O Airflow n\u00e3o \u00e9 um processador de dados e n\u00e3o pode ser utilizado para sequ\u00eancias indefinidamente c\u00edclicas de tarefas. O problema de processamento de dados nesta ferramenta pode ser mais facilmente compreendido atrav\u00e9s da explica\u00e7\u00e3o mais minuciosa do funcionamento da transfer\u00eancia de dados entre tarefas. A transfer\u00eancia de dados entre tarefas \u00e9 feita atrav\u00e9s de um componente chamado Xcom, que nada mais \u00e9 do que a abstra\u00e7\u00e3o de acesso, leitura e escrita de dados no banco de dados do Airflow. Ou seja, para cada leitura/escrita desse banco de dados, \u00e9 necess\u00e1rio fazer uma conex\u00e3o e executar uma nova opera\u00e7\u00e3o no banco, que, para grandes quantidades de dados, pode acabar resultando em problemas de consulta para outras DAGs ou tarefas que estejam sendo executadas simultaneamente. Por esse motivo, \u00e9 indicado que o processamento de dados seja feito externamente ao Airflow, utilizando Xcoms apenas para a troca de pequenas informa\u00e7\u00f5es entre tarefas, como metadados. J\u00e1 o problema de execu\u00e7\u00e3o de tarefas indefinidamente c\u00edclicas se d\u00e1 pela maneira que os diferentes componentes e sub-sistemas dentro do Airflow se comunicam. \u00c9 esperado que um bloco de tarefas (DAG) tenha um in\u00edcio bem definido e programado temporalmente, que em seguida ir\u00e1 executar suas tarefas de forma sequencial ou paralela at\u00e9 que chegue a um fim determinado, atualizando assim o estado da execu\u00e7\u00e3o desse conjunto de tarefas com a condi\u00e7\u00e3o final do mesmo (sucesso ou falha). Por esse motivo, e para evitar que usu\u00e1rios tentem executar tarefas indefinidamente, o Airflow utiliza e deixa t\u00e3o claro o conceito de grafos ac\u00edclicos dirigidos.","title":"Sobre o Apache Airflow"},{"location":"1%20-%20Introdu%C3%A7%C3%A3o%20ao%20Apache%20Airflow/Sobre%20o%20Apache%20Airflow/#sobre-o-apache-airflow","text":"Neste artigo, voc\u00ea ir\u00e1 conhecer um pouco sobre o que \u00e9 e do que se trata a ferramenta que permite automatizar a execu\u00e7\u00e3o de tarefas agendadas ( scheduled tasks ) de maneira inteligente e baseada em grafos ac\u00edclicos dirigidos ( Directed Acyclic Graph, DAG ).","title":"Sobre o Apache Airflow"},{"location":"1%20-%20Introdu%C3%A7%C3%A3o%20ao%20Apache%20Airflow/Sobre%20o%20Apache%20Airflow/#o-que-e-o-airflow","text":"O Airflow \u00e9 um software criado pelo Airbnb e escrito em Python. Se tornou um software de c\u00f3digo aberto em 2015 e logo depois foi cedida para a Apache Foundation. Seu objetivo \u00e9 orquestrar pipelines de tarefas agendadas por meio de arquivos python com instru\u00e7\u00f5es de sequenciamento definidas, chamados DAGs. Pense nele como um vers\u00e1til maestro, capaz de orquestrar diferentes m\u00fasicas, de diversos tempos e com diferentes instrumentos de maneira igualmente \u00f3tima. Para seu funcionamento, o Airflow conta com alguns elementos chave que permitem a exist\u00eancia da sinergia necess\u00e1ria entre tarefas, eventos, estados e filas, todos funcionando de maneira sincronizada e de acordo com configura\u00e7\u00f5es definidas pelo usu\u00e1rio. A Figura abaixo representa, de maneira relativamente simplificada e em uma mesma m\u00e1quina (visto que \u00e9 poss\u00edvel configurar o Airflow de maneira escalon\u00e1vel) a estrutura de uma inst\u00e2ncia da ferramenta: Sendo assim, descrevendo de maneira tamb\u00e9m simplificada a funcionalidade e comportamento de cada elemento apresentado na Figura anterior: Airflow.cfg: Arquivo de configura\u00e7\u00f5es que descreve, principalmente, as conex\u00f5es utilizadas para comunica\u00e7\u00e3o com o banco de dados de metadados da ferramenta, os intervalos de verifica\u00e7\u00e3o de novos arquivos DAGs, e a frequ\u00eancia de atualiza\u00e7\u00e3o dos estados correntes de cada tarefa. Web Server: Sub-sistema respons\u00e1vel pela integra\u00e7\u00e3o e execu\u00e7\u00e3o de uma interface visual para o usu\u00e1rio. Aqui s\u00e3o apresentados graficamente a maior parte dos elementos que podem ser utilizados pelo usu\u00e1rio, como DAGs, logs, alertas, avisos e todo tipo de monitoramento do sistema. Scheduler: Este componente pode ser entendido como o \u201ccora\u00e7\u00e3o\u201d do Airflow. No mundo musical, \u00e9 poss\u00edvel comparar o Scheduler a um metr\u00f4nomo, ou ao compasso que d\u00e1 o tempo \u00e0 m\u00fasica. Aqui, ele \u00e9 respons\u00e1vel pela temporiza\u00e7\u00e3o do sistema, resultando na execu\u00e7\u00e3o programada de DAGs, no agendamento de execu\u00e7\u00e3o de tarefas individuais das DAGs e tamb\u00e9m da distribui\u00e7\u00e3o destas para diferentes Executors. Resumidamente, garantir o bom funcionamento deste componente faz parte de um grande diferencial para garantir o bom funcionamento do Airflow como ferramenta, j\u00e1 que faz a integra\u00e7\u00e3o de quase todos os outros componentes/sub-sistemas. Metadata: Se o Scheduler pode ser considerado o \u201ccora\u00e7\u00e3o\u201d do Airflow, ent\u00e3o o banco de dados de metadados seria o \u201cc\u00e9rebro\u201d. \u00c9 neste elemento que s\u00e3o armazenadas todas as vari\u00e1veis utilizadas por todos os outros componentes da ferramenta, desde usu\u00e1rios at\u00e9 retornos de tarefas. Faz uso de um banco de dados relacional que permite, inclusive, a troca de informa\u00e7\u00e3o entre tarefas, principal causa de problemas de m\u00e1 utiliza\u00e7\u00e3o do Airflow, que ser\u00e3o discutidos no pr\u00f3ximo cap\u00edtulo. Executors: Sub-sistema respons\u00e1vel pela execu\u00e7\u00e3o das tarefas programadas pelo Scheduler, que est\u00e3o localizadas na fila (queue) deste sub-sistema (na atual representa\u00e7\u00e3o). O Airflow permite a execu\u00e7\u00e3o de diferentes tipos de tarefas atrav\u00e9s de operadores de diferentes naturezas, como o PythonOperator, para execu\u00e7\u00e3o de scripts Python, o DockerOperator, para trabalho com containers do Docker ou at\u00e9 mesmo o BashOperator, para a execu\u00e7\u00e3o de comandos bash. Essa versatilidade permite que cada tarefa possa ser executada isoladamente dentro do ambiente espec\u00edfico definido como Worker. Todas essas fun\u00e7\u00f5es s\u00e3o definidas (para a arquitetura local apresentada) e executadas dentro de um Executor, que ao fim comunica com o banco de metadados para informar o retorno das a\u00e7\u00f5es. Com a integra\u00e7\u00e3o de todos esses componentes, o usu\u00e1rio \u00e9 capaz ent\u00e3o de escrever e programar a execu\u00e7\u00e3o de diferentes conjuntos de tarefas ac\u00edclicas com uma imensa variedade de possibilidades para a execu\u00e7\u00e3o de cada tarefa, que v\u00e3o desde interpretadores Python, containers Docker e at\u00e9 mesmo comandos bash.","title":"O que \u00e9 o Airflow ?"},{"location":"1%20-%20Introdu%C3%A7%C3%A3o%20ao%20Apache%20Airflow/Sobre%20o%20Apache%20Airflow/#o-que-o-airflow-nao-e","text":"O Airflow n\u00e3o \u00e9 um processador de dados e n\u00e3o pode ser utilizado para sequ\u00eancias indefinidamente c\u00edclicas de tarefas. O problema de processamento de dados nesta ferramenta pode ser mais facilmente compreendido atrav\u00e9s da explica\u00e7\u00e3o mais minuciosa do funcionamento da transfer\u00eancia de dados entre tarefas. A transfer\u00eancia de dados entre tarefas \u00e9 feita atrav\u00e9s de um componente chamado Xcom, que nada mais \u00e9 do que a abstra\u00e7\u00e3o de acesso, leitura e escrita de dados no banco de dados do Airflow. Ou seja, para cada leitura/escrita desse banco de dados, \u00e9 necess\u00e1rio fazer uma conex\u00e3o e executar uma nova opera\u00e7\u00e3o no banco, que, para grandes quantidades de dados, pode acabar resultando em problemas de consulta para outras DAGs ou tarefas que estejam sendo executadas simultaneamente. Por esse motivo, \u00e9 indicado que o processamento de dados seja feito externamente ao Airflow, utilizando Xcoms apenas para a troca de pequenas informa\u00e7\u00f5es entre tarefas, como metadados. J\u00e1 o problema de execu\u00e7\u00e3o de tarefas indefinidamente c\u00edclicas se d\u00e1 pela maneira que os diferentes componentes e sub-sistemas dentro do Airflow se comunicam. \u00c9 esperado que um bloco de tarefas (DAG) tenha um in\u00edcio bem definido e programado temporalmente, que em seguida ir\u00e1 executar suas tarefas de forma sequencial ou paralela at\u00e9 que chegue a um fim determinado, atualizando assim o estado da execu\u00e7\u00e3o desse conjunto de tarefas com a condi\u00e7\u00e3o final do mesmo (sucesso ou falha). Por esse motivo, e para evitar que usu\u00e1rios tentem executar tarefas indefinidamente, o Airflow utiliza e deixa t\u00e3o claro o conceito de grafos ac\u00edclicos dirigidos.","title":"O que o Airflow N\u00c3O \u00c9"},{"location":"2%20-%20Entendendo%20e%20criando%20DAGs/Criac%CC%A7a%CC%83o%20da%20DAG/","text":"Cria\u00e7\u00e3o da DAG Cria\u00e7\u00e3o de DAG Primeiramente, devemos criar uma pasta chamada \u2018dags\u2019 dentro da pasta AIRFLOW_HOME , definida anteriormente no manual de instala\u00e7\u00e3o. Como no nosso manual de instala\u00e7\u00e3o utilizamos AIRFLOW_HOME=~/airflow , nossa pasta dever\u00e1 ser criada dentro da pasta airflow. mkdir ~/airflow/dags Ap\u00f3s isto, criamos um arquivo python neste diret\u00f3rio com qualquer nome, por exemplo, chamarei o arquivo de teste.py . Utilize o editor de texto de sua prefer\u00eancia, no meu caso, usarei o VS code. Em seguida, coloque o seguinte texto no arquivo: from airflow import DAG from datetime import datetime with DAG('teste', start_date = datetime(2022,5,23), schedule_interval = '30 * * * *', catchup = False) as dag: Nas duas primeiras linhas temos as importa\u00e7\u00f5es necess\u00e1rias. A terceira linha \u00e9 a cria\u00e7\u00e3o da DAG em si. Como podemos perceber, precisamos definir alguns par\u00e2metros para cria\u00e7\u00e3o da DAG. Em ordem, eles s\u00e3o: name: define o nome que aparecer\u00e1 na lista de DAGs do Airflow. No nosso caso, \u2018teste\u2019. start_date: define o in\u00edcio da execu\u00e7\u00e3o da DAG. No nosso caso, 23/05/2022. schedule_interval: define de quanto em quanto tempo a DAG deve ser executada. Este par\u00e2metro utiliza o mesmo padr\u00e3o utilizado no Crontab do sistema UNIX. Em nosso caso, de 30 em 30 minutos a DAG \u00e9 executada. catchup: define se deve executar ou n\u00e3o todas as DAGs que n\u00e3o foram executadas desde o start_date at\u00e9 o per\u00edodo atual. No nosso caso, definimos que n\u00e3o queremos executar. \ud83d\udca1 A DAG s\u00f3 \u00e9 executada automaticamente em: **tempo do start_date** + **tempo do schedule_interval**. Fique atento a isto. Entendendo Operators Um Operator \u00e9 o operador da task que iremos realizar, \u00e9 o que vai definir o meu tipo de task. Por exemplo, temos: PythonOperator, BranchPythonOperator, BashOperator etc. Isto pode ser entendido melhor na cria\u00e7\u00e3o de tasks. Criando uma task Continuando o c\u00f3digo anterior, acrescentamos alguns comandos: from airflow import DAG from datetime import datetime from airflow.operators.python import PythonOperator def hello(): print(\"Hello world\") with DAG('teste', start_date = datetime(2022,5,23), schedule_interval = '30 * * * *', catchup = False) as dag: helloWorld = PythonOperator( task_id = 'Hello_World', python_callable = hello ) Perceba que dentro do bloco da DAG criada (vari\u00e1vel \u2018dag\u2019), definimos uma vari\u00e1vel chamada \u2018helloWorld\u2019. Esta vari\u00e1vel referencia uma task do tipo PythonOperator, o que significa que esta task em espec\u00edfico ir\u00e1 rodar instru\u00e7\u00f5es python. Dentro do PythonOperator devemos definir dois par\u00e2metros: task_id: Identificador da task. python_callable: Fun\u00e7\u00e3o python que ser\u00e1 executada nessa task. Perceba que a fun\u00e7\u00e3o python \u2018hello\u2019 (que referenciamos) est\u00e1 definida mais acima no c\u00f3digo. Ela \u00e9 uma fun\u00e7\u00e3o python normal que printa na tela \u201cHello World\u201d. Finalmente, s\u00f3 temos que definir mais uma coisa para podermos executar esta DAG. Devemos informar a ordem de execu\u00e7\u00e3o das tarefas. E fazemos isto no final. Como neste exemplo s\u00f3 temos uma tarefa que ser\u00e1 executada, devemos apenas informar o nome dela dentro do c\u00f3digo. E assim, a DAG fica: from airflow import DAG from datetime import datetime from airflow.operators.python import PythonOperator def hello(): print(\"Hello world\") with DAG('teste', start_date = datetime(2022,5,23), schedule_interval = '30 * * * *', catchup = False) as dag: helloWorld = PythonOperator( task_id = 'Hello_World', python_callable = hello ) helloWorld \ud83d\udca1 Perceba que no final temos o nome \u2018helloWorld\u2019 dentro do bloco da DAG. Executando Para visualizarmos nossa DAG dentro do airflow, basta atualizarmos a tela inicial do programa. Perceba que a DAG \u2018teste\u2019 j\u00e1 aparece na lista. Ap\u00f3s clicar em nossa DAG e execut\u00e1-la, podemos clicar no \u2018Log\u2019 da nossa task e assim visualizar de fato o que foi executado. Perceba a sa\u00edda Hello world na tela. No pr\u00f3ximo artigo iremos criar uma DAG um pouco mais complexa.","title":"Cria\u00e7\u00e3o da DAG"},{"location":"2%20-%20Entendendo%20e%20criando%20DAGs/Criac%CC%A7a%CC%83o%20da%20DAG/#criacao-da-dag","text":"","title":"Cria\u00e7\u00e3o da DAG"},{"location":"2%20-%20Entendendo%20e%20criando%20DAGs/Criac%CC%A7a%CC%83o%20da%20DAG/#criacao-de-dag","text":"Primeiramente, devemos criar uma pasta chamada \u2018dags\u2019 dentro da pasta AIRFLOW_HOME , definida anteriormente no manual de instala\u00e7\u00e3o. Como no nosso manual de instala\u00e7\u00e3o utilizamos AIRFLOW_HOME=~/airflow , nossa pasta dever\u00e1 ser criada dentro da pasta airflow. mkdir ~/airflow/dags Ap\u00f3s isto, criamos um arquivo python neste diret\u00f3rio com qualquer nome, por exemplo, chamarei o arquivo de teste.py . Utilize o editor de texto de sua prefer\u00eancia, no meu caso, usarei o VS code. Em seguida, coloque o seguinte texto no arquivo: from airflow import DAG from datetime import datetime with DAG('teste', start_date = datetime(2022,5,23), schedule_interval = '30 * * * *', catchup = False) as dag: Nas duas primeiras linhas temos as importa\u00e7\u00f5es necess\u00e1rias. A terceira linha \u00e9 a cria\u00e7\u00e3o da DAG em si. Como podemos perceber, precisamos definir alguns par\u00e2metros para cria\u00e7\u00e3o da DAG. Em ordem, eles s\u00e3o: name: define o nome que aparecer\u00e1 na lista de DAGs do Airflow. No nosso caso, \u2018teste\u2019. start_date: define o in\u00edcio da execu\u00e7\u00e3o da DAG. No nosso caso, 23/05/2022. schedule_interval: define de quanto em quanto tempo a DAG deve ser executada. Este par\u00e2metro utiliza o mesmo padr\u00e3o utilizado no Crontab do sistema UNIX. Em nosso caso, de 30 em 30 minutos a DAG \u00e9 executada. catchup: define se deve executar ou n\u00e3o todas as DAGs que n\u00e3o foram executadas desde o start_date at\u00e9 o per\u00edodo atual. No nosso caso, definimos que n\u00e3o queremos executar. \ud83d\udca1 A DAG s\u00f3 \u00e9 executada automaticamente em: **tempo do start_date** + **tempo do schedule_interval**. Fique atento a isto.","title":"Cria\u00e7\u00e3o de DAG"},{"location":"2%20-%20Entendendo%20e%20criando%20DAGs/Criac%CC%A7a%CC%83o%20da%20DAG/#entendendo-operators","text":"Um Operator \u00e9 o operador da task que iremos realizar, \u00e9 o que vai definir o meu tipo de task. Por exemplo, temos: PythonOperator, BranchPythonOperator, BashOperator etc. Isto pode ser entendido melhor na cria\u00e7\u00e3o de tasks.","title":"Entendendo Operators"},{"location":"2%20-%20Entendendo%20e%20criando%20DAGs/Criac%CC%A7a%CC%83o%20da%20DAG/#criando-uma-task","text":"Continuando o c\u00f3digo anterior, acrescentamos alguns comandos: from airflow import DAG from datetime import datetime from airflow.operators.python import PythonOperator def hello(): print(\"Hello world\") with DAG('teste', start_date = datetime(2022,5,23), schedule_interval = '30 * * * *', catchup = False) as dag: helloWorld = PythonOperator( task_id = 'Hello_World', python_callable = hello ) Perceba que dentro do bloco da DAG criada (vari\u00e1vel \u2018dag\u2019), definimos uma vari\u00e1vel chamada \u2018helloWorld\u2019. Esta vari\u00e1vel referencia uma task do tipo PythonOperator, o que significa que esta task em espec\u00edfico ir\u00e1 rodar instru\u00e7\u00f5es python. Dentro do PythonOperator devemos definir dois par\u00e2metros: task_id: Identificador da task. python_callable: Fun\u00e7\u00e3o python que ser\u00e1 executada nessa task. Perceba que a fun\u00e7\u00e3o python \u2018hello\u2019 (que referenciamos) est\u00e1 definida mais acima no c\u00f3digo. Ela \u00e9 uma fun\u00e7\u00e3o python normal que printa na tela \u201cHello World\u201d. Finalmente, s\u00f3 temos que definir mais uma coisa para podermos executar esta DAG. Devemos informar a ordem de execu\u00e7\u00e3o das tarefas. E fazemos isto no final. Como neste exemplo s\u00f3 temos uma tarefa que ser\u00e1 executada, devemos apenas informar o nome dela dentro do c\u00f3digo. E assim, a DAG fica: from airflow import DAG from datetime import datetime from airflow.operators.python import PythonOperator def hello(): print(\"Hello world\") with DAG('teste', start_date = datetime(2022,5,23), schedule_interval = '30 * * * *', catchup = False) as dag: helloWorld = PythonOperator( task_id = 'Hello_World', python_callable = hello ) helloWorld \ud83d\udca1 Perceba que no final temos o nome \u2018helloWorld\u2019 dentro do bloco da DAG.","title":"Criando uma task"},{"location":"2%20-%20Entendendo%20e%20criando%20DAGs/Criac%CC%A7a%CC%83o%20da%20DAG/#executando","text":"Para visualizarmos nossa DAG dentro do airflow, basta atualizarmos a tela inicial do programa. Perceba que a DAG \u2018teste\u2019 j\u00e1 aparece na lista. Ap\u00f3s clicar em nossa DAG e execut\u00e1-la, podemos clicar no \u2018Log\u2019 da nossa task e assim visualizar de fato o que foi executado. Perceba a sa\u00edda Hello world na tela. No pr\u00f3ximo artigo iremos criar uma DAG um pouco mais complexa.","title":"Executando"},{"location":"2%20-%20Entendendo%20e%20criando%20DAGs/DAGs%20no%20Apache%20Airflow/","text":"DAGs no Apache Airflow Primeiramente, o que s\u00e3o DAGs? Em primeiro lugar, devemos saber que DAG n\u00e3o \u00e9 um conceito novo. Na verdade, as DAGs s\u00e3o conhecidas desde o desenvolvimento das Teorias de grafos em matem\u00e1tica e que mais tarde foram usadas na computa\u00e7\u00e3o devido \u00e0 sua enorme utilidade neste campo. Quando falamos sobre DAG, estamos a falar sobre grafos com duas propriedades muito interessantes: s\u00e3o dirigidos e ac\u00edclicos . Em primeiro lugar, um grafo \u00e9 direcionado, quando todos os n\u00f3s (ou v\u00e9rtices) que fazem parte do grafo s\u00e3o conectados por arestas que indicam uma dire\u00e7\u00e3o bem definida . Segundo, falamos de um grafo ac\u00edclico, quando estamos diante de um grafo onde n\u00e3o h\u00e1 ciclos de deslocamento para o mesmo. Por outras palavras, \u00e9 imposs\u00edvel ir de um v\u00e9rtice do grafo, passar pelos demais v\u00e9rtices e terminar no v\u00e9rtice onde a jornada come\u00e7ou. Assim, as DAGs t\u00eam certas propriedades que s\u00e3o vitais para que funcionem bem: T\u00eam um ponto de partida (origem) e um ponto de chegada ou final. Ao serem direcionados, isso garante que a nossa rota vai sempre de um ponto de origem a um ponto final, e n\u00e3o podemos retornar por essa rota. Se a constru\u00e7\u00e3o dessa estrutura for aplicada consecutivamente, estaremos a criar um hist\u00f3rico incremental dentro da DAG, como acontece em blockchain. Modificar uma rela\u00e7\u00e3o entre v\u00e9rtices reescreve toda a DAG, porque a sua estrutura e peso foram alterados. Isto \u00e9 equivalente a se modificarmos um bloco na blockchain, o resultado ser\u00e1 uma blockchain diferente daquele ponto em diante. S\u00e3o paraleliz\u00e1veis. Uma DAG pode ter gera\u00e7\u00e3o paralela e caminhos de valores diferentes entre v\u00e9rtices diferentes. Isto otimiza a sua gera\u00e7\u00e3o e a capacidade de verificar a rela\u00e7\u00e3o entre os v\u00e9rtices e as informa\u00e7\u00f5es que eles podem conter. S\u00e3o redut\u00edveis. Uma propriedade exclusiva das DAGs \u00e9 que sua estrutura pode ser reduzida a um ponto ideal onde seu caminho atende a todos os relacionamentos especificados nele sem qualquer perda. Basicamente, significa que \u00e9 poss\u00edvel reduzir as rela\u00e7\u00f5es dos v\u00e9rtices (ou blocos) a um ponto m\u00ednimo onde tal redu\u00e7\u00e3o n\u00e3o afete a capacidade de verificar a informa\u00e7\u00e3o de qualquer v\u00e9rtice a qualquer momento. Isso \u00e9 especialmente \u00fatil. Entendendo as DAGs no Airflow Abrindo o software Airflow, temos uma p\u00e1gina semelhante a esta. Cada um destes elementos da lista representa uma DAG diferente. Como exemplo, clicaremos na DAG \u2018tutorial\u2019 e na aba Graph View (para termos uma visualiza\u00e7\u00e3o em gr\u00e1fico da nossa DAG). Ent\u00e3o teremos a seguinte tela. Perceba que temos 3 blocos em nossa DAG tutorial: print_date, sleep e templated . Cada um destes blocos representa uma task . Task \u00e9 alguma tarefa fundamental a ser executada para que o processo todo seja conclu\u00eddo. \u00c9 uma unidade para constru\u00e7\u00e3o do todo. Pelo gr\u00e1fico tamb\u00e9m podemos visualizar a ordem de execu\u00e7\u00e3o das tasks. Temos a execu\u00e7\u00e3o da task print_date e em seguida as execu\u00e7\u00f5es de sleep e templated . Se execut\u00e1ssemos a DAG, ver\u00edamos esta sequ\u00eancia de execu\u00e7\u00e3o atrav\u00e9s do c\u00f3digo de cores mostrado acima. As principais cores que devemos nos atentar na execu\u00e7\u00e3o de cada task s\u00e3o as seguintes: Cinza: Tarefa est\u00e1 na fila de execu\u00e7\u00e3o. Verde claro: Tarefa est\u00e1 executando. Verde escuro: Tarefa completou a sua execu\u00e7\u00e3o com sucesso. Vermelho: A execu\u00e7\u00e3o da tarefa falhou em algum ponto. Al\u00e9m disso, existem diferentes tipos de Tasks. Estas tasks podem ser uma execu\u00e7\u00e3o de uma fun\u00e7\u00e3o python, podem ser a execu\u00e7\u00e3o de algum comando no terminal e outros. Veremos estes detalhes mais adiante. Execu\u00e7\u00e3o das DAGs Para executar uma DAG, podemos clicar no bot\u00e3o na parte superior esquerda do programa. Para acompanharmos mais detalhadamente a execu\u00e7\u00e3o de alguma Task, podemos clicar na Task que queremos analisar. Em seguida, clicar no bot\u00e3o \u2018Log\u2019. E ent\u00e3o, teremos uma visualiza\u00e7\u00e3o mais detalhada do que est\u00e1 acontecendo. Nesta parte, podemos acompanhar os erros que alguma Task pode ter disparado tamb\u00e9m. Beleza. Ap\u00f3s aprendermos o que s\u00e3o as DAGs e como elas podem ser visualizadas e manipuladas no Airflow, agora, podemos criar a nossa primeira DAG.","title":"DAGs no Apache Airflow"},{"location":"2%20-%20Entendendo%20e%20criando%20DAGs/DAGs%20no%20Apache%20Airflow/#dags-no-apache-airflow","text":"","title":"DAGs no Apache Airflow"},{"location":"2%20-%20Entendendo%20e%20criando%20DAGs/DAGs%20no%20Apache%20Airflow/#primeiramente-o-que-sao-dags","text":"Em primeiro lugar, devemos saber que DAG n\u00e3o \u00e9 um conceito novo. Na verdade, as DAGs s\u00e3o conhecidas desde o desenvolvimento das Teorias de grafos em matem\u00e1tica e que mais tarde foram usadas na computa\u00e7\u00e3o devido \u00e0 sua enorme utilidade neste campo. Quando falamos sobre DAG, estamos a falar sobre grafos com duas propriedades muito interessantes: s\u00e3o dirigidos e ac\u00edclicos . Em primeiro lugar, um grafo \u00e9 direcionado, quando todos os n\u00f3s (ou v\u00e9rtices) que fazem parte do grafo s\u00e3o conectados por arestas que indicam uma dire\u00e7\u00e3o bem definida . Segundo, falamos de um grafo ac\u00edclico, quando estamos diante de um grafo onde n\u00e3o h\u00e1 ciclos de deslocamento para o mesmo. Por outras palavras, \u00e9 imposs\u00edvel ir de um v\u00e9rtice do grafo, passar pelos demais v\u00e9rtices e terminar no v\u00e9rtice onde a jornada come\u00e7ou. Assim, as DAGs t\u00eam certas propriedades que s\u00e3o vitais para que funcionem bem: T\u00eam um ponto de partida (origem) e um ponto de chegada ou final. Ao serem direcionados, isso garante que a nossa rota vai sempre de um ponto de origem a um ponto final, e n\u00e3o podemos retornar por essa rota. Se a constru\u00e7\u00e3o dessa estrutura for aplicada consecutivamente, estaremos a criar um hist\u00f3rico incremental dentro da DAG, como acontece em blockchain. Modificar uma rela\u00e7\u00e3o entre v\u00e9rtices reescreve toda a DAG, porque a sua estrutura e peso foram alterados. Isto \u00e9 equivalente a se modificarmos um bloco na blockchain, o resultado ser\u00e1 uma blockchain diferente daquele ponto em diante. S\u00e3o paraleliz\u00e1veis. Uma DAG pode ter gera\u00e7\u00e3o paralela e caminhos de valores diferentes entre v\u00e9rtices diferentes. Isto otimiza a sua gera\u00e7\u00e3o e a capacidade de verificar a rela\u00e7\u00e3o entre os v\u00e9rtices e as informa\u00e7\u00f5es que eles podem conter. S\u00e3o redut\u00edveis. Uma propriedade exclusiva das DAGs \u00e9 que sua estrutura pode ser reduzida a um ponto ideal onde seu caminho atende a todos os relacionamentos especificados nele sem qualquer perda. Basicamente, significa que \u00e9 poss\u00edvel reduzir as rela\u00e7\u00f5es dos v\u00e9rtices (ou blocos) a um ponto m\u00ednimo onde tal redu\u00e7\u00e3o n\u00e3o afete a capacidade de verificar a informa\u00e7\u00e3o de qualquer v\u00e9rtice a qualquer momento. Isso \u00e9 especialmente \u00fatil.","title":"Primeiramente, o que s\u00e3o DAGs?"},{"location":"2%20-%20Entendendo%20e%20criando%20DAGs/DAGs%20no%20Apache%20Airflow/#entendendo-as-dags-no-airflow","text":"Abrindo o software Airflow, temos uma p\u00e1gina semelhante a esta. Cada um destes elementos da lista representa uma DAG diferente. Como exemplo, clicaremos na DAG \u2018tutorial\u2019 e na aba Graph View (para termos uma visualiza\u00e7\u00e3o em gr\u00e1fico da nossa DAG). Ent\u00e3o teremos a seguinte tela. Perceba que temos 3 blocos em nossa DAG tutorial: print_date, sleep e templated . Cada um destes blocos representa uma task . Task \u00e9 alguma tarefa fundamental a ser executada para que o processo todo seja conclu\u00eddo. \u00c9 uma unidade para constru\u00e7\u00e3o do todo. Pelo gr\u00e1fico tamb\u00e9m podemos visualizar a ordem de execu\u00e7\u00e3o das tasks. Temos a execu\u00e7\u00e3o da task print_date e em seguida as execu\u00e7\u00f5es de sleep e templated . Se execut\u00e1ssemos a DAG, ver\u00edamos esta sequ\u00eancia de execu\u00e7\u00e3o atrav\u00e9s do c\u00f3digo de cores mostrado acima. As principais cores que devemos nos atentar na execu\u00e7\u00e3o de cada task s\u00e3o as seguintes: Cinza: Tarefa est\u00e1 na fila de execu\u00e7\u00e3o. Verde claro: Tarefa est\u00e1 executando. Verde escuro: Tarefa completou a sua execu\u00e7\u00e3o com sucesso. Vermelho: A execu\u00e7\u00e3o da tarefa falhou em algum ponto. Al\u00e9m disso, existem diferentes tipos de Tasks. Estas tasks podem ser uma execu\u00e7\u00e3o de uma fun\u00e7\u00e3o python, podem ser a execu\u00e7\u00e3o de algum comando no terminal e outros. Veremos estes detalhes mais adiante.","title":"Entendendo as DAGs no Airflow"},{"location":"2%20-%20Entendendo%20e%20criando%20DAGs/DAGs%20no%20Apache%20Airflow/#execucao-das-dags","text":"Para executar uma DAG, podemos clicar no bot\u00e3o na parte superior esquerda do programa. Para acompanharmos mais detalhadamente a execu\u00e7\u00e3o de alguma Task, podemos clicar na Task que queremos analisar. Em seguida, clicar no bot\u00e3o \u2018Log\u2019. E ent\u00e3o, teremos uma visualiza\u00e7\u00e3o mais detalhada do que est\u00e1 acontecendo. Nesta parte, podemos acompanhar os erros que alguma Task pode ter disparado tamb\u00e9m. Beleza. Ap\u00f3s aprendermos o que s\u00e3o as DAGs e como elas podem ser visualizadas e manipuladas no Airflow, agora, podemos criar a nossa primeira DAG.","title":"Execu\u00e7\u00e3o das DAGs"},{"location":"2%20-%20Entendendo%20e%20criando%20DAGs/Exerc%C3%ADcio%2002/","text":"Exerc\u00edcio 02 J\u00e1 que agora voc\u00ea entendeu o que \u00e9 uma DAG e como manipul\u00e1-la de forma b\u00e1sica no Airflow, podemos fazer alguns exerc\u00edcios. DAG para conselho di\u00e1rio A Advide Slip \u00e9 uma API que lhe retorna um conselho para a sua vida. Exemplo, ao fazer a seguinte requisi\u00e7\u00e3o: https://api.adviceslip.com/advice Temos o retorno de algum conselho aleat\u00f3rio da API: { \"slip\": { \"id\": 202, \"advice\": \"Never waste an opportunity to tell someone you love them.\" } } A sua tarefa \u00e9 criar uma DAG com uma tarefa apenas, que fa\u00e7a uma requisi\u00e7\u00e3o para esta API e retorne um conselho todo dia as 6:30h da manh\u00e3. Voc\u00ea ter\u00e1 que pesquisar sobre o crontab do sistema UNIX para descrever o par\u00e2metro schedule_interval corretamente. Eis um breve resumo de funcionamento. \u26a0\ufe0f Recomendamos utilizar o racioc\u00ednio do exerc\u00edcio acima para criar DAGs com algum tipo de agendamento. Pense em alguma informa\u00e7\u00e3o que voc\u00ea possa querer saber em algum momento do dia e implemente para ser executada na hora e nos dias que desejar.","title":"Exerc\u00edcio 02"},{"location":"2%20-%20Entendendo%20e%20criando%20DAGs/Exerc%C3%ADcio%2002/#exercicio-02","text":"J\u00e1 que agora voc\u00ea entendeu o que \u00e9 uma DAG e como manipul\u00e1-la de forma b\u00e1sica no Airflow, podemos fazer alguns exerc\u00edcios.","title":"Exerc\u00edcio 02"},{"location":"2%20-%20Entendendo%20e%20criando%20DAGs/Exerc%C3%ADcio%2002/#dag-para-conselho-diario","text":"A Advide Slip \u00e9 uma API que lhe retorna um conselho para a sua vida. Exemplo, ao fazer a seguinte requisi\u00e7\u00e3o: https://api.adviceslip.com/advice Temos o retorno de algum conselho aleat\u00f3rio da API: { \"slip\": { \"id\": 202, \"advice\": \"Never waste an opportunity to tell someone you love them.\" } } A sua tarefa \u00e9 criar uma DAG com uma tarefa apenas, que fa\u00e7a uma requisi\u00e7\u00e3o para esta API e retorne um conselho todo dia as 6:30h da manh\u00e3. Voc\u00ea ter\u00e1 que pesquisar sobre o crontab do sistema UNIX para descrever o par\u00e2metro schedule_interval corretamente. Eis um breve resumo de funcionamento. \u26a0\ufe0f Recomendamos utilizar o racioc\u00ednio do exerc\u00edcio acima para criar DAGs com algum tipo de agendamento. Pense em alguma informa\u00e7\u00e3o que voc\u00ea possa querer saber em algum momento do dia e implemente para ser executada na hora e nos dias que desejar.","title":"DAG para conselho di\u00e1rio"},{"location":"2%20-%20Entendendo%20e%20criando%20DAGs/Resoluc%CC%A7a%CC%83o%2002/","text":"Resolu\u00e7\u00e3o 02 Uma poss\u00edvel resolu\u00e7\u00e3o para o Exerc\u00edcio 02: from airflow import DAG import datetime as dt from airflow.operators.python import PythonOperator import requests import json def captura_conta_dados(): url = \"https://api.adviceslip.com/advice\" response = requests.get(url) print(response) todos = json.loads(response.content) print(\"\\n\\n\") print(todos['slip']['advice']) print(\"\\n\\n\") with DAG('exerc\u00edcio2', start_date = dt.datetime(2022,8,16), schedule_interval = '30 6 * * *', catchup = False) as dag: Captura_dados = PythonOperator( task_id = 'Capturar_dados', python_callable = captura_conta_dados ) Captura_dados","title":"Resolu\u00e7\u00e3o 02"},{"location":"2%20-%20Entendendo%20e%20criando%20DAGs/Resoluc%CC%A7a%CC%83o%2002/#resolucao-02","text":"Uma poss\u00edvel resolu\u00e7\u00e3o para o Exerc\u00edcio 02: from airflow import DAG import datetime as dt from airflow.operators.python import PythonOperator import requests import json def captura_conta_dados(): url = \"https://api.adviceslip.com/advice\" response = requests.get(url) print(response) todos = json.loads(response.content) print(\"\\n\\n\") print(todos['slip']['advice']) print(\"\\n\\n\") with DAG('exerc\u00edcio2', start_date = dt.datetime(2022,8,16), schedule_interval = '30 6 * * *', catchup = False) as dag: Captura_dados = PythonOperator( task_id = 'Capturar_dados', python_callable = captura_conta_dados ) Captura_dados","title":"Resolu\u00e7\u00e3o 02"},{"location":"3%20-%20Criando%20uma%20DAG%20mais%20complexa/Definindo%20DAG%20como%20soluc%CC%A7a%CC%83o/","text":"Definindo DAG como solu\u00e7\u00e3o Para desenvolvermos a solu\u00e7\u00e3o, precisamos construir cada uma de nossas Tasks. Task 1 Claramente, a task 1 ir\u00e1 utilizar um PythonOperator, pois pode ser feita atrav\u00e9s de uma fun\u00e7\u00e3o Python. Pensando na fun\u00e7\u00e3o, podemos ter algo semelhante a isto. def captura_conta_dados(): url = \"https://api.github.com/users/Daniel-Alencar/repos\" response = requests.get(url) df = pd.json_normalize(json.loads(response.content)) quantidade = len(df.index) return quantidade A API do GitHub nos disponibiliza os reposit\u00f3rios que temos em nossa conta. Assim, definimos a URL que iremos acessar para conseguir estes dados. Ap\u00f3s fazermos o requerimento destes dados, o padronizamos para deixarmos de uma maneira que podemos acess\u00e1-lo facilmente e logo em seguida recuperamos o n\u00famero de reposit\u00f3rios na vari\u00e1vel quantidade. Em seguida, retornamos este valor na fun\u00e7\u00e3o. Observe que devemos fazer as seguintes importa\u00e7\u00f5es para a fun\u00e7\u00e3o rodar perfeitamente. import pandas as pd import requests import json Criando a nossa tarefa dentro da DAG, podemos ter algo semelhante a isto: # Task de execu\u00e7\u00e3o de scripts python T1 = PythonOperator( task_id = 'task_1', python_callable = captura_conta_dados ) Task 2 A segunda task tamb\u00e9m parece ser uma execu\u00e7\u00e3o python, por\u00e9m, devemos dar alguma indica\u00e7\u00e3o da pr\u00f3xima tarefa que iremos executar ap\u00f3s finalizar a task 2 (pois, dependendo do valor, ou a task 3 ou a task 4 ser\u00e1 executada). Logo, devemos utilizar o BranchPythonOperator desta vez. Como iremos analisar o valor retornado da task anterior, teremos algo semelhante a isto. def isValido(task_instance): quantidade = task_instance.xcom_pull(task_ids = 'task_1') # Especifica a pr\u00f3xima task a ser realizada if (quantidade > 30): return 'task_3' return 'task_4' A execu\u00e7\u00e3o task_instance.xcom_pull(task_ids = 'task 1') retorna o valor que foi retornado da \u2018task_1\u2019, que no caso \u00e9 a quantidade de reposit\u00f3rios no GitHub. \ud83d\udca1 Tamb\u00e9m \u00e9 necess\u00e1rio termos `task_instance` como argumento desta fun\u00e7\u00e3o. Assim, conseguirmos recuperar o valor de quantidade sem problemas. Tendo esta quantidade, analisamos se ela \u00e9 maior do que 30. Se sim, devemos executar a \u2018task_3\u2019, e por isso, retornamos o ID da task 3. Se n\u00e3o, devemos executar a \u2018task_4\u2019, e por isso, retornamos o ID da task 4. A Task ser\u00e1 definida mais ou menos desta forma (mesma ideia dos par\u00e2metros da task anterior): # Task de execu\u00e7\u00e3o de scripts python + Escolha da pr\u00f3xima task a ser realizada T2 = BranchPythonOperator( task_id = 'task_2', python_callable = isValido ) Task 3 e Task 4 Digamos que eu queira que apare\u00e7a na tela \u201cQuantidade OK\u201d e \u201cQuantidade n\u00e3o OK\u201d para a task 3 e task 4, respectivamente. Desta vez, utilizaremos outro tipo de operator, o BashOperator. Pois, queremos que a amostragem se d\u00ea por meio de um comando de terminal Bash. Assim, ter\u00edamos algo semelhante a isto para a task 3 e task 4. # Task de execu\u00e7\u00e3o de comando no terminal Bash T3 = BashOperator( task_id = 'task_3', bash_command = 'echo \"Quantidade OK\"' ) # Task de execu\u00e7\u00e3o de comando no terminal Bash T4 = BashOperator( task_id = 'task_4', bash_command = 'echo \"Quantidade n\u00e3o OK\"' ) Sendo: task_id: Identifica\u00e7\u00e3o da task. bash_command: Comando do Bash para ser executado. Definindo a ordem de execu\u00e7\u00e3o task 1 task 2 task 3 ou task 4 No c\u00f3digo, podemos especificar isto da seguinte forma: # Define a ordem de execu\u00e7\u00e3o das tasks T1 >> T2 >> [T3, T4] Finalizando Organizando tudo teremos algo assim. from airflow import DAG from datetime import datetime from airflow.operators.python import PythonOperator, BranchPythonOperator from airflow.operators.bash import BashOperator import pandas as pd import requests import json def captura_conta_dados(): url = \"https://api.github.com/users/Daniel-Alencar/repos\" response = requests.get(url) df = pd.json_normalize(json.loads(response.content)) quantidade = len(df.index) return quantidade def isValido(task_instance): quantidade = task_instance.xcom_pull(task_ids = 'task_1') # Especifica a pr\u00f3xima task a ser realizada if (quantidade > 30): return 'task_3' return 'task_4' with DAG('new_teste', start_date = datetime(2022,5,23), schedule_interval = '30 * * * *', catchup = False) as dag: # Task de execu\u00e7\u00e3o de scripts python T1 = PythonOperator( task_id = 'task_1', python_callable = captura_conta_dados ) # Task de execu\u00e7\u00e3o de scripts python + Escolha da pr\u00f3xima task a ser realizada T2 = BranchPythonOperator( task_id = 'task_2', python_callable = isValido ) # Task de execu\u00e7\u00e3o de comando no terminal Bash T3 = BashOperator( task_id = 'task_3', bash_command = 'echo \"Quantidade OK\"' ) # Task de execu\u00e7\u00e3o de comando no terminal Bash T4 = BashOperator( task_id = 'task_4', bash_command = 'echo \"Quantidade n\u00e3o OK\"' ) # Define a ordem de execu\u00e7\u00e3o das tasks T1 >> T2 >> [T3, T4] Execu\u00e7\u00e3o da DAG criada Observe que no final temos a execu\u00e7\u00e3o da task_4. E est\u00e1 correto, pois de fato, n\u00e3o tenho mais de 30 reposit\u00f3rios em meu GitHub. Analisando o \u2018Log\u2019 da task_4, podemos visualizar a mensagem \u201cQuantidade n\u00e3o OK\u201d. Atrav\u00e9s deste exemplo, entendemos melhor o processo de cria\u00e7\u00e3o de DAGs no Airflow.","title":"Definindo DAG como soluc\u0327a\u0303o"},{"location":"3%20-%20Criando%20uma%20DAG%20mais%20complexa/Definindo%20DAG%20como%20soluc%CC%A7a%CC%83o/#definindo-dag-como-solucao","text":"Para desenvolvermos a solu\u00e7\u00e3o, precisamos construir cada uma de nossas Tasks.","title":"Definindo DAG como solu\u00e7\u00e3o"},{"location":"3%20-%20Criando%20uma%20DAG%20mais%20complexa/Definindo%20DAG%20como%20soluc%CC%A7a%CC%83o/#task-1","text":"Claramente, a task 1 ir\u00e1 utilizar um PythonOperator, pois pode ser feita atrav\u00e9s de uma fun\u00e7\u00e3o Python. Pensando na fun\u00e7\u00e3o, podemos ter algo semelhante a isto. def captura_conta_dados(): url = \"https://api.github.com/users/Daniel-Alencar/repos\" response = requests.get(url) df = pd.json_normalize(json.loads(response.content)) quantidade = len(df.index) return quantidade A API do GitHub nos disponibiliza os reposit\u00f3rios que temos em nossa conta. Assim, definimos a URL que iremos acessar para conseguir estes dados. Ap\u00f3s fazermos o requerimento destes dados, o padronizamos para deixarmos de uma maneira que podemos acess\u00e1-lo facilmente e logo em seguida recuperamos o n\u00famero de reposit\u00f3rios na vari\u00e1vel quantidade. Em seguida, retornamos este valor na fun\u00e7\u00e3o. Observe que devemos fazer as seguintes importa\u00e7\u00f5es para a fun\u00e7\u00e3o rodar perfeitamente. import pandas as pd import requests import json Criando a nossa tarefa dentro da DAG, podemos ter algo semelhante a isto: # Task de execu\u00e7\u00e3o de scripts python T1 = PythonOperator( task_id = 'task_1', python_callable = captura_conta_dados )","title":"Task 1"},{"location":"3%20-%20Criando%20uma%20DAG%20mais%20complexa/Definindo%20DAG%20como%20soluc%CC%A7a%CC%83o/#task-2","text":"A segunda task tamb\u00e9m parece ser uma execu\u00e7\u00e3o python, por\u00e9m, devemos dar alguma indica\u00e7\u00e3o da pr\u00f3xima tarefa que iremos executar ap\u00f3s finalizar a task 2 (pois, dependendo do valor, ou a task 3 ou a task 4 ser\u00e1 executada). Logo, devemos utilizar o BranchPythonOperator desta vez. Como iremos analisar o valor retornado da task anterior, teremos algo semelhante a isto. def isValido(task_instance): quantidade = task_instance.xcom_pull(task_ids = 'task_1') # Especifica a pr\u00f3xima task a ser realizada if (quantidade > 30): return 'task_3' return 'task_4' A execu\u00e7\u00e3o task_instance.xcom_pull(task_ids = 'task 1') retorna o valor que foi retornado da \u2018task_1\u2019, que no caso \u00e9 a quantidade de reposit\u00f3rios no GitHub. \ud83d\udca1 Tamb\u00e9m \u00e9 necess\u00e1rio termos `task_instance` como argumento desta fun\u00e7\u00e3o. Assim, conseguirmos recuperar o valor de quantidade sem problemas. Tendo esta quantidade, analisamos se ela \u00e9 maior do que 30. Se sim, devemos executar a \u2018task_3\u2019, e por isso, retornamos o ID da task 3. Se n\u00e3o, devemos executar a \u2018task_4\u2019, e por isso, retornamos o ID da task 4. A Task ser\u00e1 definida mais ou menos desta forma (mesma ideia dos par\u00e2metros da task anterior): # Task de execu\u00e7\u00e3o de scripts python + Escolha da pr\u00f3xima task a ser realizada T2 = BranchPythonOperator( task_id = 'task_2', python_callable = isValido )","title":"Task 2"},{"location":"3%20-%20Criando%20uma%20DAG%20mais%20complexa/Definindo%20DAG%20como%20soluc%CC%A7a%CC%83o/#task-3-e-task-4","text":"Digamos que eu queira que apare\u00e7a na tela \u201cQuantidade OK\u201d e \u201cQuantidade n\u00e3o OK\u201d para a task 3 e task 4, respectivamente. Desta vez, utilizaremos outro tipo de operator, o BashOperator. Pois, queremos que a amostragem se d\u00ea por meio de um comando de terminal Bash. Assim, ter\u00edamos algo semelhante a isto para a task 3 e task 4. # Task de execu\u00e7\u00e3o de comando no terminal Bash T3 = BashOperator( task_id = 'task_3', bash_command = 'echo \"Quantidade OK\"' ) # Task de execu\u00e7\u00e3o de comando no terminal Bash T4 = BashOperator( task_id = 'task_4', bash_command = 'echo \"Quantidade n\u00e3o OK\"' ) Sendo: task_id: Identifica\u00e7\u00e3o da task. bash_command: Comando do Bash para ser executado.","title":"Task 3 e Task 4"},{"location":"3%20-%20Criando%20uma%20DAG%20mais%20complexa/Definindo%20DAG%20como%20soluc%CC%A7a%CC%83o/#definindo-a-ordem-de-execucao","text":"task 1 task 2 task 3 ou task 4 No c\u00f3digo, podemos especificar isto da seguinte forma: # Define a ordem de execu\u00e7\u00e3o das tasks T1 >> T2 >> [T3, T4]","title":"Definindo a ordem de execu\u00e7\u00e3o"},{"location":"3%20-%20Criando%20uma%20DAG%20mais%20complexa/Definindo%20DAG%20como%20soluc%CC%A7a%CC%83o/#finalizando","text":"Organizando tudo teremos algo assim. from airflow import DAG from datetime import datetime from airflow.operators.python import PythonOperator, BranchPythonOperator from airflow.operators.bash import BashOperator import pandas as pd import requests import json def captura_conta_dados(): url = \"https://api.github.com/users/Daniel-Alencar/repos\" response = requests.get(url) df = pd.json_normalize(json.loads(response.content)) quantidade = len(df.index) return quantidade def isValido(task_instance): quantidade = task_instance.xcom_pull(task_ids = 'task_1') # Especifica a pr\u00f3xima task a ser realizada if (quantidade > 30): return 'task_3' return 'task_4' with DAG('new_teste', start_date = datetime(2022,5,23), schedule_interval = '30 * * * *', catchup = False) as dag: # Task de execu\u00e7\u00e3o de scripts python T1 = PythonOperator( task_id = 'task_1', python_callable = captura_conta_dados ) # Task de execu\u00e7\u00e3o de scripts python + Escolha da pr\u00f3xima task a ser realizada T2 = BranchPythonOperator( task_id = 'task_2', python_callable = isValido ) # Task de execu\u00e7\u00e3o de comando no terminal Bash T3 = BashOperator( task_id = 'task_3', bash_command = 'echo \"Quantidade OK\"' ) # Task de execu\u00e7\u00e3o de comando no terminal Bash T4 = BashOperator( task_id = 'task_4', bash_command = 'echo \"Quantidade n\u00e3o OK\"' ) # Define a ordem de execu\u00e7\u00e3o das tasks T1 >> T2 >> [T3, T4]","title":"Finalizando"},{"location":"3%20-%20Criando%20uma%20DAG%20mais%20complexa/Definindo%20DAG%20como%20soluc%CC%A7a%CC%83o/#execucao-da-dag-criada","text":"Observe que no final temos a execu\u00e7\u00e3o da task_4. E est\u00e1 correto, pois de fato, n\u00e3o tenho mais de 30 reposit\u00f3rios em meu GitHub. Analisando o \u2018Log\u2019 da task_4, podemos visualizar a mensagem \u201cQuantidade n\u00e3o OK\u201d. Atrav\u00e9s deste exemplo, entendemos melhor o processo de cria\u00e7\u00e3o de DAGs no Airflow.","title":"Execu\u00e7\u00e3o da DAG criada"},{"location":"3%20-%20Criando%20uma%20DAG%20mais%20complexa/Definindo%20problema/","text":"Definindo problema Vista a cria\u00e7\u00e3o de uma DAG simples. Podemos criar uma DAG um pouco mais complexa para visualizarmos a sequ\u00eancia de tasks a serem executadas. Comecemos de onde paramos. Copie o c\u00f3digo da DAG que foi criada no artigo anterior para um outro arquivo que irei chamar de new_teste . Este arquivo deve estar dentro da pasta \u2018dags\u2019 (pasta que foi criada anteriormente para armazenar DAGs criadas). Assim, temos o seguinte c\u00f3digo em nosso arquivo. from airflow import DAG from datetime import datetime from airflow.operators.python import PythonOperator def hello(): print(\"Hello world\") with DAG('teste', start_date = datetime(2022,5,23), schedule_interval = '30 * * * *', catchup = False) as dag: helloWorld = PythonOperator( task_id = 'Hello_World', python_callable = hello ) helloWorld Por\u00e9m, desta vez, deixaremos somente a parte da DAG criada (mudando apenas o par\u00e2metro \u2018name\u2019 para o nome \u2018new_teste\u2019). Assim, ficamos com isto em nosso c\u00f3digo: from airflow import DAG from datetime import datetime with DAG('new_teste', start_date = datetime(2022,5,23), schedule_interval = '30 * * * *', catchup = False) as dag: Definindo um problema Digamos que seja muito importante eu saber se a quantidade de reposit\u00f3rios no meu Github \u00e9 maior do que algum determinado valor, digamos, 30. Logo, seria interessante termos uma DAG que cuide disso para mim. Perceba que podemos dividir o problema em mini-problemas que seriam as nossas tasks . Uma poss\u00edvel divis\u00e3o de tasks seria: Task 1: Busca a quantidade de reposit\u00f3rios que eu tenho no Github. Task 2: Analisa esta quantidade. Task 3: Faz alguma coisa (caso a quantidade seja maior). Task 4: Faz alguma outra coisa (caso a quantidade seja menor). Primeiro executamos a Task 1, logo em seguida a Task 2 (pois depende da primeira task). E com base na an\u00e1lise do valor na segunda Task, devo executar ou a Task 3 ou Task 4. Na pr\u00f3xima aula iremos construir a solu\u00e7\u00e3o para o problema apresentado.","title":"Definindo problema"},{"location":"3%20-%20Criando%20uma%20DAG%20mais%20complexa/Definindo%20problema/#definindo-problema","text":"Vista a cria\u00e7\u00e3o de uma DAG simples. Podemos criar uma DAG um pouco mais complexa para visualizarmos a sequ\u00eancia de tasks a serem executadas. Comecemos de onde paramos. Copie o c\u00f3digo da DAG que foi criada no artigo anterior para um outro arquivo que irei chamar de new_teste . Este arquivo deve estar dentro da pasta \u2018dags\u2019 (pasta que foi criada anteriormente para armazenar DAGs criadas). Assim, temos o seguinte c\u00f3digo em nosso arquivo. from airflow import DAG from datetime import datetime from airflow.operators.python import PythonOperator def hello(): print(\"Hello world\") with DAG('teste', start_date = datetime(2022,5,23), schedule_interval = '30 * * * *', catchup = False) as dag: helloWorld = PythonOperator( task_id = 'Hello_World', python_callable = hello ) helloWorld Por\u00e9m, desta vez, deixaremos somente a parte da DAG criada (mudando apenas o par\u00e2metro \u2018name\u2019 para o nome \u2018new_teste\u2019). Assim, ficamos com isto em nosso c\u00f3digo: from airflow import DAG from datetime import datetime with DAG('new_teste', start_date = datetime(2022,5,23), schedule_interval = '30 * * * *', catchup = False) as dag:","title":"Definindo problema"},{"location":"3%20-%20Criando%20uma%20DAG%20mais%20complexa/Definindo%20problema/#definindo-um-problema","text":"Digamos que seja muito importante eu saber se a quantidade de reposit\u00f3rios no meu Github \u00e9 maior do que algum determinado valor, digamos, 30. Logo, seria interessante termos uma DAG que cuide disso para mim. Perceba que podemos dividir o problema em mini-problemas que seriam as nossas tasks . Uma poss\u00edvel divis\u00e3o de tasks seria: Task 1: Busca a quantidade de reposit\u00f3rios que eu tenho no Github. Task 2: Analisa esta quantidade. Task 3: Faz alguma coisa (caso a quantidade seja maior). Task 4: Faz alguma outra coisa (caso a quantidade seja menor). Primeiro executamos a Task 1, logo em seguida a Task 2 (pois depende da primeira task). E com base na an\u00e1lise do valor na segunda Task, devo executar ou a Task 3 ou Task 4. Na pr\u00f3xima aula iremos construir a solu\u00e7\u00e3o para o problema apresentado.","title":"Definindo um problema"},{"location":"3%20-%20Criando%20uma%20DAG%20mais%20complexa/Exerc%C3%ADcio%2003/","text":"Exerc\u00edcio 03 Agora que voc\u00ea entendeu melhor como utilizar muitas tarefas em sua DAG, podemos resolver exerc\u00edcios um pouco mais elaborados. M\u00e9dia dos alunos Fa\u00e7a uma DAG que calcule a m\u00e9dia dos alunos de uma turma. A notas dos alunos devem estar em um arquivo CSV, como no exemplo abaixo: 10,8,5 7,4,6 8,2,0 0,0,0 10,5,0 7,8,9 Onde cada linha representa as notas de um aluno e cada nota \u00e9 separada por v\u00edrgula na linha. Voc\u00ea deve ler corretamente este arquivo e a partir disso, calcular a m\u00e9dia de cada aluno e apresentar em outro arquivo CSV semelhante (n\u00e3o necessariamente no mesmo formato) ao exemplo abaixo: 7.66 5.66 3.33 0 5 8 \u26a0\ufe0f Utilize quantas tarefas achar necess\u00e1rio!","title":"Exerc\u00edcio 03"},{"location":"3%20-%20Criando%20uma%20DAG%20mais%20complexa/Exerc%C3%ADcio%2003/#exercicio-03","text":"Agora que voc\u00ea entendeu melhor como utilizar muitas tarefas em sua DAG, podemos resolver exerc\u00edcios um pouco mais elaborados.","title":"Exerc\u00edcio 03"},{"location":"3%20-%20Criando%20uma%20DAG%20mais%20complexa/Exerc%C3%ADcio%2003/#media-dos-alunos","text":"Fa\u00e7a uma DAG que calcule a m\u00e9dia dos alunos de uma turma. A notas dos alunos devem estar em um arquivo CSV, como no exemplo abaixo: 10,8,5 7,4,6 8,2,0 0,0,0 10,5,0 7,8,9 Onde cada linha representa as notas de um aluno e cada nota \u00e9 separada por v\u00edrgula na linha. Voc\u00ea deve ler corretamente este arquivo e a partir disso, calcular a m\u00e9dia de cada aluno e apresentar em outro arquivo CSV semelhante (n\u00e3o necessariamente no mesmo formato) ao exemplo abaixo: 7.66 5.66 3.33 0 5 8 \u26a0\ufe0f Utilize quantas tarefas achar necess\u00e1rio!","title":"M\u00e9dia dos alunos"},{"location":"3%20-%20Criando%20uma%20DAG%20mais%20complexa/Resoluc%CC%A7a%CC%83o%2003/","text":"Resolu\u00e7\u00e3o 03 Uma poss\u00edvel resolu\u00e7\u00e3o para o Exerc\u00edcio 03: from airflow import DAG import datetime as dt import csv from airflow.operators.python import PythonOperator def calcular_medias(): medias = [] with open('notas.csv') as ficheiro: reader = csv.reader(ficheiro) for linha in reader: soma = 0 for coluna in linha: soma += int(coluna) medias.append(soma / 3) print(medias) with open('./medias.csv', 'w') as csvfile: for media in medias: csv.writer(csvfile, delimiter=',').writerow([media]) with DAG('exerc\u00edcio3', start_date = dt.datetime(2022,8,16), schedule_interval = '0 9 * * *', catchup = False) as dag: Calcular_medias = PythonOperator( task_id = 'Calcular_medias', python_callable = calcular_medias ) Calcular_medias \u26a0\ufe0f Para se desafiar um pouco, tente resolver o problema utilizando mais de uma Task!","title":"Resolu\u00e7\u00e3o 03"},{"location":"3%20-%20Criando%20uma%20DAG%20mais%20complexa/Resoluc%CC%A7a%CC%83o%2003/#resolucao-03","text":"Uma poss\u00edvel resolu\u00e7\u00e3o para o Exerc\u00edcio 03: from airflow import DAG import datetime as dt import csv from airflow.operators.python import PythonOperator def calcular_medias(): medias = [] with open('notas.csv') as ficheiro: reader = csv.reader(ficheiro) for linha in reader: soma = 0 for coluna in linha: soma += int(coluna) medias.append(soma / 3) print(medias) with open('./medias.csv', 'w') as csvfile: for media in medias: csv.writer(csvfile, delimiter=',').writerow([media]) with DAG('exerc\u00edcio3', start_date = dt.datetime(2022,8,16), schedule_interval = '0 9 * * *', catchup = False) as dag: Calcular_medias = PythonOperator( task_id = 'Calcular_medias', python_callable = calcular_medias ) Calcular_medias \u26a0\ufe0f Para se desafiar um pouco, tente resolver o problema utilizando mais de uma Task!","title":"Resolu\u00e7\u00e3o 03"},{"location":"4%20-%20Agendamento%20e%20Sensores/Agendamento/","text":"Agendamento Introdu\u00e7\u00e3o DAGs s\u00e3o disparadas de duas formas: Manualmente por meio da UI ou API. Automaticamente por meio de uma pol\u00edtica de disparo. J\u00e1 para que as DAGs sejam \"dispar\u00e1veis\" (i.e. execut\u00e1veis) elas precisam de: Uma data de in\u00edcio a partir do qual podem ser executadas. Uma pol\u00edtica de disparo que pode ser (al\u00e9m do disparo manual): um intervalo de execu\u00e7\u00e3o e/ou um crit\u00e9rio condicional. Essas configura\u00e7\u00f5es s\u00e3o definidas durante a inicializa\u00e7\u00e3o (i.e. cria\u00e7\u00e3o do objeto) da DAG. Configurando o Agendamento de DAGs Chamamos de agendamento (ou escalonamento) o procedimento executado pelo Airflow de definir quando uma DAG deve ser executada. O agendamento \u00e9 configurado atrav\u00e9s de tr\u00eas par\u00e2metros principais: start_date , schedule_interval e end_date . Todos estes par\u00e2metros s\u00e3o atribuidos durante a inicializa\u00e7\u00e3o da DAG. start_date [obrigat\u00f3rio] . Momento a partir do qual a DAG em quest\u00e3o estar\u00e1 dispon\u00edvel para ser executada. Note que o argumento start_date \u00e9 obrigat\u00f3rio durante a inicializa\u00e7\u00e3o pois sem uma data de in\u00edcio \u00e9 imposs\u00edvel para o Airflow saber se a DAG pode ou n\u00e3o ser executada. schedule_interval . Intervalo de execu\u00e7\u00e3o da DAG. Por padr\u00e3o, o valor de schedule_interval \u00e9 None , o que significa que a DAG em quest\u00e3o s\u00f3 ser\u00e1 executada quando triggada manualmente. end_date . Momento at\u00e9 onde a DAG deve ser executada. Abaixo, um exemplo de uma DAG com escalonamento di\u00e1rio. dag = DAG( dag_id=\"02_daily_schedule\", schedule_interval=\"@daily\", start_date=dt.datetime(2019, 1, 1), ... ) Uma vez definida a DAG, o Airflow ir\u00e1 agendar sua primeira execu\u00e7\u00e3o para o primeiro intervalo a partir da data de in\u00edcio. \u26a0\ufe0f Se definirmos que uma DAG est\u00e1 dispon\u00edvel para ser executada a partir do dia **09 de Agosto de 2021** \u00e0s **00hrs** com **intervalo de execu\u00e7\u00e3o de 15 minutos**, a primeira execu\u00e7\u00e3o ser\u00e1 agendada para 00:15, a segunda execu\u00e7\u00e3o ser\u00e1 agendada para 00:30 e assim sucessivamente. Se n\u00e3o definirmos uma data final, o Airflow ir\u00e1 agendar e executar a DAG em quest\u00e3o eternamente. Assim, caso exista uma data final definitiva a partir do qual a DAG n\u00e3o dever\u00e1 ser executada, podemos utilizar o argumento end_date da mesma forma que start_date para limitar os agendamentos. dag = DAG( dag_id=\"03_with_end_date\", schedule_interval=\"@daily\", start_date=dt.datetime(year=2019, month=1, day=1), end_date=dt.datetime(year=2019, month=1, day=5), ) Intervalos Baseados em Cron Podemos definir intervalos de execu\u00e7\u00e3o complexos usando a mesma sintaxe que usamos no cron . Basicamente, a sintaxe \u00e9 composta por cinco componentes organizados da seguinte forma: O car\u00e1cter * significa que o valor do campo em quest\u00e3o n\u00e3o importa. Assim, podemos definir desde intervalos simples e convencionais: 0 * * * * . Executa a cada hora At\u00e9 intervalos mais complexos 0 0 1 * * . Executa a cada primeiro dia do m\u00eas (\u00e0s 00hrs) Tamb\u00e9m podemos utilizar de v\u00edrgulas ( , ) para definir conjuntos de valores e h\u00edfen ( - ) para intervalos de valores. Por exemplo: 0 0 * * MON, WED, FRI . Executa toda segunda, quarta e sexta-feira (\u00e0s 00hrs) 0 0,12 * * MON-FRI . Executa \u00e0s 00hrs e 12hrs de segunda \u00e0 sexta-feira Alternativamente, podemos recorrer \u00e0 ferramentas como crontab.guru e crontab-generator para definirmos as express\u00f5es de forma mais f\u00e1cil. O Airflow tamb\u00e9m fornece alguns macros que podemos utilizar com mais facilidade. Os mais comuns s\u00e3o: Macro Descri\u00e7\u00e3o @once Executa uma \u00fanica vez @hourly Executa a cada come\u00e7o de hora @daily Executa todos os dias \u00e0s 00hrs @weekly Executa todo domingo \u00e0s 00hrs Intervalos Baseados em Frequ\u00eancia Embora poderosas, express\u00f5es cron s\u00e3o incapazes de representar agendamentos baseados em frequ\u00eancia. Por exemplo, n\u00e3o \u00e9 poss\u00edvel definir (de forma adequada) um intervalo de \"tr\u00eas em tr\u00eas em dias\". Por conta disso, o Airflow tamb\u00e9m aceita inst\u00e2ncias timedelta para definir intervalos de execu\u00e7\u00e3o. Com isso, podemos definir uma DAG que \u00e9 executada a cada tr\u00eas dias a partir da data de in\u00edcio. dag = DAG( dag_id=\"04_time_delta\", schedule_interval=dt.timedelta(days=3), start_date=dt.datetime(year=2019, month=1, day=1), end_date=dt.datetime(year=2019, month=1, day=5), ) Catchup & Backfill Por padr\u00e3o, o Airflow sempre ir\u00e1 escalonar e executar toda e qualquer execu\u00e7\u00e3o passada que deveria ter sido executada mas que por algum motivo n\u00e3o foi . Este comportamento \u00e9 denominado \"backfill\" e \u00e9 controlado pelo argumento catchup presente na inicializa\u00e7\u00e3o da DAG. Caso este comportamento n\u00e3o seja desejado, basta desativ\u00e1-lo atribuindo False ao par\u00e2metro. Dessa forma, o Airflow ir\u00e1 executar a DAG apenas a partir do primeiro intervalo de execu\u00e7\u00e3o mais pr\u00f3ximo. dag = DAG( dag_id=\"09_no_catchup\", schedule_interval=\"@daily\", start_date=dt.datetime(year=2019, month=1, day=1), end_date=dt.datetime(year=2019, month=1, day=5), catchup=False, ) Exemplo: Se uma DAG for ativada dentro de um per\u00edodo intervalar, o Airflow ir\u00e1 agendar a execu\u00e7\u00e3o da DAG para o in\u00edcio deste per\u00edodo. Por exemplo, considerando uma DAG com start_date=datetime(2021, 08, 10) e schedule_interval=\"@daily\" ; se ativarmos a DAG no dia 15 do mesmo m\u00eas \u00e0s 12hrs, o Airflow ir\u00e1 definir que a primeira execu\u00e7\u00e3o da DAG dever\u00e1 ter ocorrido \u00e0s 00hrs do dia 15 e ir\u00e1 executar a DAG imediatamente. \u26a0\ufe0f Para mudar o valor padr\u00e3o do `catchup` de `True` para `False`, basta acessar o arquivo de configura\u00e7\u00e3o e modificar o par\u00e2metro `catchup_by_default`. Embora o backfilling possa ser indesejado em algumas situa\u00e7\u00f5es, seu uso \u00e9 muito \u00fatil para a reexecu\u00e7\u00e3o de tarefas hist\u00f3ricas. Por exemplo, suponha as seguintes tarefas download_data >> process_data . Considerando que os dados adquiridos atrav\u00e9s da tarefa download_data ainda sejam reacess\u00e1veis localmente, podemos realizar as altera\u00e7\u00f5es desejadas em process_data e ent\u00e3o limparmos as execu\u00e7\u00f5es passadas (atrav\u00e9s do bot\u00e3o Clear) para que assim o Airflow reagende e execute a nova implementa\u00e7\u00e3o de process_data . Aten\u00e7\u00e3o: A reexecu\u00e7\u00e3o das DAGs n\u00e3o ocorre de forma ordenada (i.e. de acordo com a data de execu\u00e7\u00e3o), mas sim de forma paralela. Para que o backfilling ocorra de forma ordenada, \u00e9 necess\u00e1rio que o argumento depends_on_past presente na inicializa\u00e7\u00e3o das tarefas seja True . Detalhes sobre o argumento ser\u00e3o apresentados a diante. Datas de execu\u00e7\u00e3o de uma DAG Em diversas situa\u00e7\u00f5es \u00e9 \u00fatil sabermos as datas de execu\u00e7\u00e3o de uma DAG. Por conta disso, o Airflow nos permite acessar tanto a data da execu\u00e7\u00e3o corrente da DAG, quanto a data da execu\u00e7\u00e3o imediatamente anterior e posterior. execution_date. Data da execu\u00e7\u00e3o corrente de DAG. Contudo, diferente do que o nome sugere, a data de execu\u00e7\u00e3o marcada na DAG n\u00e3o \u00e9 a data em que ela foi executada, mas sim no momento em que ela deveria ser executada, com base no intervalo de execu\u00e7\u00e3o. Exemplo: Suponha que temos uma DAG configurada para executar diariamente a partir do dia 2020/01/01. Ap\u00f3s dez dias de execu\u00e7\u00e3o, fizemos algumas altera\u00e7\u00f5es de implementa\u00e7\u00e3o e agora precisamos que as execu\u00e7\u00f5es dos \u00faltimos dez dias sejam refeitas. Neste caso, ao acionarmos a op\u00e7\u00e3o Clear, as datas de execu\u00e7\u00e3o da DAG permanecer\u00e3o de acordo com a data em que foram agendadas originalmente. Agora, se triggarmos manualmente a DAG antes da pr\u00f3xima execu\u00e7\u00e3o agendada, a execution_date ser\u00e1 o momento em que a DAG foi de fato disparada. previous_execution_date . Data de execu\u00e7\u00e3o ( execution_date ) da DAG anterior. next_execution_date . Pr\u00f3xima data de execu\u00e7\u00e3o ( execution_date ) agendada para a DAG. O acesso a essas informa\u00e7\u00f5es pode ser feito atrav\u00e9s do contexto da DAG[^1] ou por meio dos macros pr\u00e9-definidos: Macros: Macro Descri\u00e7\u00e3o {{ ds }} Data de execu\u00e7\u00e3o no formato YYYY-MM-DD {{ ds_nodash }} Data de execu\u00e7\u00e3o no formato YYYYMMDD {{ prev_ds }} Data de execu\u00e7\u00e3o anterior no formato YYYY-MM-DD {{ next_ds }} Pr\u00f3xima data de execu\u00e7\u00e3o no formato YYYY-MM-DD Uso: fetch_events = BashOperator( task_id=\"fetch_events\", bash_command=( f\"curl -o /data/events.json {source}\" \"start_date={{ ds }}\" \"end_date={{ next_ds }}\" ), ... ) Observa\u00e7\u00f5es O material desta aula foi fortemente baseado neste artigo .","title":"Agendamento"},{"location":"4%20-%20Agendamento%20e%20Sensores/Agendamento/#agendamento","text":"","title":"Agendamento"},{"location":"4%20-%20Agendamento%20e%20Sensores/Agendamento/#introducao","text":"DAGs s\u00e3o disparadas de duas formas: Manualmente por meio da UI ou API. Automaticamente por meio de uma pol\u00edtica de disparo. J\u00e1 para que as DAGs sejam \"dispar\u00e1veis\" (i.e. execut\u00e1veis) elas precisam de: Uma data de in\u00edcio a partir do qual podem ser executadas. Uma pol\u00edtica de disparo que pode ser (al\u00e9m do disparo manual): um intervalo de execu\u00e7\u00e3o e/ou um crit\u00e9rio condicional. Essas configura\u00e7\u00f5es s\u00e3o definidas durante a inicializa\u00e7\u00e3o (i.e. cria\u00e7\u00e3o do objeto) da DAG.","title":"Introdu\u00e7\u00e3o"},{"location":"4%20-%20Agendamento%20e%20Sensores/Agendamento/#configurando-o-agendamento-de-dags","text":"Chamamos de agendamento (ou escalonamento) o procedimento executado pelo Airflow de definir quando uma DAG deve ser executada. O agendamento \u00e9 configurado atrav\u00e9s de tr\u00eas par\u00e2metros principais: start_date , schedule_interval e end_date . Todos estes par\u00e2metros s\u00e3o atribuidos durante a inicializa\u00e7\u00e3o da DAG. start_date [obrigat\u00f3rio] . Momento a partir do qual a DAG em quest\u00e3o estar\u00e1 dispon\u00edvel para ser executada. Note que o argumento start_date \u00e9 obrigat\u00f3rio durante a inicializa\u00e7\u00e3o pois sem uma data de in\u00edcio \u00e9 imposs\u00edvel para o Airflow saber se a DAG pode ou n\u00e3o ser executada. schedule_interval . Intervalo de execu\u00e7\u00e3o da DAG. Por padr\u00e3o, o valor de schedule_interval \u00e9 None , o que significa que a DAG em quest\u00e3o s\u00f3 ser\u00e1 executada quando triggada manualmente. end_date . Momento at\u00e9 onde a DAG deve ser executada. Abaixo, um exemplo de uma DAG com escalonamento di\u00e1rio. dag = DAG( dag_id=\"02_daily_schedule\", schedule_interval=\"@daily\", start_date=dt.datetime(2019, 1, 1), ... ) Uma vez definida a DAG, o Airflow ir\u00e1 agendar sua primeira execu\u00e7\u00e3o para o primeiro intervalo a partir da data de in\u00edcio. \u26a0\ufe0f Se definirmos que uma DAG est\u00e1 dispon\u00edvel para ser executada a partir do dia **09 de Agosto de 2021** \u00e0s **00hrs** com **intervalo de execu\u00e7\u00e3o de 15 minutos**, a primeira execu\u00e7\u00e3o ser\u00e1 agendada para 00:15, a segunda execu\u00e7\u00e3o ser\u00e1 agendada para 00:30 e assim sucessivamente. Se n\u00e3o definirmos uma data final, o Airflow ir\u00e1 agendar e executar a DAG em quest\u00e3o eternamente. Assim, caso exista uma data final definitiva a partir do qual a DAG n\u00e3o dever\u00e1 ser executada, podemos utilizar o argumento end_date da mesma forma que start_date para limitar os agendamentos. dag = DAG( dag_id=\"03_with_end_date\", schedule_interval=\"@daily\", start_date=dt.datetime(year=2019, month=1, day=1), end_date=dt.datetime(year=2019, month=1, day=5), )","title":"Configurando o Agendamento de DAGs"},{"location":"4%20-%20Agendamento%20e%20Sensores/Agendamento/#intervalos-baseados-em-cron","text":"Podemos definir intervalos de execu\u00e7\u00e3o complexos usando a mesma sintaxe que usamos no cron . Basicamente, a sintaxe \u00e9 composta por cinco componentes organizados da seguinte forma: O car\u00e1cter * significa que o valor do campo em quest\u00e3o n\u00e3o importa. Assim, podemos definir desde intervalos simples e convencionais: 0 * * * * . Executa a cada hora At\u00e9 intervalos mais complexos 0 0 1 * * . Executa a cada primeiro dia do m\u00eas (\u00e0s 00hrs) Tamb\u00e9m podemos utilizar de v\u00edrgulas ( , ) para definir conjuntos de valores e h\u00edfen ( - ) para intervalos de valores. Por exemplo: 0 0 * * MON, WED, FRI . Executa toda segunda, quarta e sexta-feira (\u00e0s 00hrs) 0 0,12 * * MON-FRI . Executa \u00e0s 00hrs e 12hrs de segunda \u00e0 sexta-feira Alternativamente, podemos recorrer \u00e0 ferramentas como crontab.guru e crontab-generator para definirmos as express\u00f5es de forma mais f\u00e1cil. O Airflow tamb\u00e9m fornece alguns macros que podemos utilizar com mais facilidade. Os mais comuns s\u00e3o: Macro Descri\u00e7\u00e3o @once Executa uma \u00fanica vez @hourly Executa a cada come\u00e7o de hora @daily Executa todos os dias \u00e0s 00hrs @weekly Executa todo domingo \u00e0s 00hrs","title":"Intervalos Baseados em Cron"},{"location":"4%20-%20Agendamento%20e%20Sensores/Agendamento/#intervalos-baseados-em-frequencia","text":"Embora poderosas, express\u00f5es cron s\u00e3o incapazes de representar agendamentos baseados em frequ\u00eancia. Por exemplo, n\u00e3o \u00e9 poss\u00edvel definir (de forma adequada) um intervalo de \"tr\u00eas em tr\u00eas em dias\". Por conta disso, o Airflow tamb\u00e9m aceita inst\u00e2ncias timedelta para definir intervalos de execu\u00e7\u00e3o. Com isso, podemos definir uma DAG que \u00e9 executada a cada tr\u00eas dias a partir da data de in\u00edcio. dag = DAG( dag_id=\"04_time_delta\", schedule_interval=dt.timedelta(days=3), start_date=dt.datetime(year=2019, month=1, day=1), end_date=dt.datetime(year=2019, month=1, day=5), )","title":"Intervalos Baseados em Frequ\u00eancia"},{"location":"4%20-%20Agendamento%20e%20Sensores/Agendamento/#catchup-backfill","text":"Por padr\u00e3o, o Airflow sempre ir\u00e1 escalonar e executar toda e qualquer execu\u00e7\u00e3o passada que deveria ter sido executada mas que por algum motivo n\u00e3o foi . Este comportamento \u00e9 denominado \"backfill\" e \u00e9 controlado pelo argumento catchup presente na inicializa\u00e7\u00e3o da DAG. Caso este comportamento n\u00e3o seja desejado, basta desativ\u00e1-lo atribuindo False ao par\u00e2metro. Dessa forma, o Airflow ir\u00e1 executar a DAG apenas a partir do primeiro intervalo de execu\u00e7\u00e3o mais pr\u00f3ximo. dag = DAG( dag_id=\"09_no_catchup\", schedule_interval=\"@daily\", start_date=dt.datetime(year=2019, month=1, day=1), end_date=dt.datetime(year=2019, month=1, day=5), catchup=False, ) Exemplo: Se uma DAG for ativada dentro de um per\u00edodo intervalar, o Airflow ir\u00e1 agendar a execu\u00e7\u00e3o da DAG para o in\u00edcio deste per\u00edodo. Por exemplo, considerando uma DAG com start_date=datetime(2021, 08, 10) e schedule_interval=\"@daily\" ; se ativarmos a DAG no dia 15 do mesmo m\u00eas \u00e0s 12hrs, o Airflow ir\u00e1 definir que a primeira execu\u00e7\u00e3o da DAG dever\u00e1 ter ocorrido \u00e0s 00hrs do dia 15 e ir\u00e1 executar a DAG imediatamente. \u26a0\ufe0f Para mudar o valor padr\u00e3o do `catchup` de `True` para `False`, basta acessar o arquivo de configura\u00e7\u00e3o e modificar o par\u00e2metro `catchup_by_default`. Embora o backfilling possa ser indesejado em algumas situa\u00e7\u00f5es, seu uso \u00e9 muito \u00fatil para a reexecu\u00e7\u00e3o de tarefas hist\u00f3ricas. Por exemplo, suponha as seguintes tarefas download_data >> process_data . Considerando que os dados adquiridos atrav\u00e9s da tarefa download_data ainda sejam reacess\u00e1veis localmente, podemos realizar as altera\u00e7\u00f5es desejadas em process_data e ent\u00e3o limparmos as execu\u00e7\u00f5es passadas (atrav\u00e9s do bot\u00e3o Clear) para que assim o Airflow reagende e execute a nova implementa\u00e7\u00e3o de process_data . Aten\u00e7\u00e3o: A reexecu\u00e7\u00e3o das DAGs n\u00e3o ocorre de forma ordenada (i.e. de acordo com a data de execu\u00e7\u00e3o), mas sim de forma paralela. Para que o backfilling ocorra de forma ordenada, \u00e9 necess\u00e1rio que o argumento depends_on_past presente na inicializa\u00e7\u00e3o das tarefas seja True . Detalhes sobre o argumento ser\u00e3o apresentados a diante.","title":"Catchup &amp; Backfill"},{"location":"4%20-%20Agendamento%20e%20Sensores/Agendamento/#datas-de-execucao-de-uma-dag","text":"Em diversas situa\u00e7\u00f5es \u00e9 \u00fatil sabermos as datas de execu\u00e7\u00e3o de uma DAG. Por conta disso, o Airflow nos permite acessar tanto a data da execu\u00e7\u00e3o corrente da DAG, quanto a data da execu\u00e7\u00e3o imediatamente anterior e posterior. execution_date. Data da execu\u00e7\u00e3o corrente de DAG. Contudo, diferente do que o nome sugere, a data de execu\u00e7\u00e3o marcada na DAG n\u00e3o \u00e9 a data em que ela foi executada, mas sim no momento em que ela deveria ser executada, com base no intervalo de execu\u00e7\u00e3o. Exemplo: Suponha que temos uma DAG configurada para executar diariamente a partir do dia 2020/01/01. Ap\u00f3s dez dias de execu\u00e7\u00e3o, fizemos algumas altera\u00e7\u00f5es de implementa\u00e7\u00e3o e agora precisamos que as execu\u00e7\u00f5es dos \u00faltimos dez dias sejam refeitas. Neste caso, ao acionarmos a op\u00e7\u00e3o Clear, as datas de execu\u00e7\u00e3o da DAG permanecer\u00e3o de acordo com a data em que foram agendadas originalmente. Agora, se triggarmos manualmente a DAG antes da pr\u00f3xima execu\u00e7\u00e3o agendada, a execution_date ser\u00e1 o momento em que a DAG foi de fato disparada. previous_execution_date . Data de execu\u00e7\u00e3o ( execution_date ) da DAG anterior. next_execution_date . Pr\u00f3xima data de execu\u00e7\u00e3o ( execution_date ) agendada para a DAG. O acesso a essas informa\u00e7\u00f5es pode ser feito atrav\u00e9s do contexto da DAG[^1] ou por meio dos macros pr\u00e9-definidos: Macros: Macro Descri\u00e7\u00e3o {{ ds }} Data de execu\u00e7\u00e3o no formato YYYY-MM-DD {{ ds_nodash }} Data de execu\u00e7\u00e3o no formato YYYYMMDD {{ prev_ds }} Data de execu\u00e7\u00e3o anterior no formato YYYY-MM-DD {{ next_ds }} Pr\u00f3xima data de execu\u00e7\u00e3o no formato YYYY-MM-DD Uso: fetch_events = BashOperator( task_id=\"fetch_events\", bash_command=( f\"curl -o /data/events.json {source}\" \"start_date={{ ds }}\" \"end_date={{ next_ds }}\" ), ... )","title":"Datas de execu\u00e7\u00e3o de uma DAG"},{"location":"4%20-%20Agendamento%20e%20Sensores/Agendamento/#observacoes","text":"O material desta aula foi fortemente baseado neste artigo .","title":"Observa\u00e7\u00f5es"},{"location":"4%20-%20Agendamento%20e%20Sensores/Exerc%C3%ADcio%2004/","text":"Exerc\u00edcio 04 Aprimoramento de m\u00e9dia dos alunos Como deve ter percebido, este ser\u00e1 um exerc\u00edcio para aprimoramento da solu\u00e7\u00e3o do exerc\u00edcio anterior. Voc\u00ea deve primeiramente verificar se existe uma arquivo CSV no diret\u00f3rio antes de efetivamente fazer a m\u00e9dia dos alunos (com FileSensor). Regule esta verifica\u00e7\u00e3o para ser feita de 5 em 5 segundos (caso o arquivo n\u00e3o exista). Com o timeout de 5 minutos. Caso o arquivo notas.csv exista no diret\u00f3rio, prossiga normalmente: Recolha as notas dos alunos. Calcule a m\u00e9dia de cada aluno. Preencha um arquivo CSV com as m\u00e9dias dos alunos.","title":"Exerc\u00edcio 04"},{"location":"4%20-%20Agendamento%20e%20Sensores/Exerc%C3%ADcio%2004/#exercicio-04","text":"","title":"Exerc\u00edcio 04"},{"location":"4%20-%20Agendamento%20e%20Sensores/Exerc%C3%ADcio%2004/#aprimoramento-de-media-dos-alunos","text":"Como deve ter percebido, este ser\u00e1 um exerc\u00edcio para aprimoramento da solu\u00e7\u00e3o do exerc\u00edcio anterior. Voc\u00ea deve primeiramente verificar se existe uma arquivo CSV no diret\u00f3rio antes de efetivamente fazer a m\u00e9dia dos alunos (com FileSensor). Regule esta verifica\u00e7\u00e3o para ser feita de 5 em 5 segundos (caso o arquivo n\u00e3o exista). Com o timeout de 5 minutos. Caso o arquivo notas.csv exista no diret\u00f3rio, prossiga normalmente: Recolha as notas dos alunos. Calcule a m\u00e9dia de cada aluno. Preencha um arquivo CSV com as m\u00e9dias dos alunos.","title":"Aprimoramento de m\u00e9dia dos alunos"},{"location":"4%20-%20Agendamento%20e%20Sensores/Resoluc%CC%A7a%CC%83o%2004/","text":"Resolu\u00e7\u00e3o 04 Poss\u00edvel resolu\u00e7\u00e3o para o Exerc\u00edcio 04: from airflow import DAG import datetime as dt import csv from airflow.operators.python import PythonOperator from airflow.sensors.filesystem import FileSensor from airflow.operators.bash import BashOperator def calcular_medias(): medias = [] with open('notas.csv') as ficheiro: reader = csv.reader(ficheiro) for linha in reader: soma = 0 for coluna in linha: soma += int(coluna) medias.append(soma / 3) return medias def fazer_CSV(task_instance): medias = task_instance.xcom_pull(task_ids = 'Calcular_medias') print(medias) with open('./medias.csv', 'w') as csvfile: for media in medias: csv.writer(csvfile, delimiter=',').writerow([media]) with DAG('exerc\u00edcio4', start_date = dt.datetime(2022,8,16), schedule_interval = '0 9 * * *', catchup = False) as dag: Wait_for_file = FileSensor( task_id = \"wait_for_file\", filepath = \"/home/MEGA/Documents/Projetos pessoais/IC/airflow-test/notas.csv\", poke_interval = 5, timeout = 5 * 60, ) Calcular_medias = PythonOperator( task_id = 'Calcular_medias', python_callable = calcular_medias ) Fazer_CSV = PythonOperator( task_id = 'Fazer_CSV', python_callable = fazer_CSV ) Message = BashOperator( task_id = 'Message', bash_command = 'echo \"As m\u00e9dias foram calculadas com sucesso!\"' ) Wait_for_file >> Calcular_medias >> Fazer_CSV >> Message","title":"Resolu\u00e7\u00e3o 04"},{"location":"4%20-%20Agendamento%20e%20Sensores/Resoluc%CC%A7a%CC%83o%2004/#resolucao-04","text":"Poss\u00edvel resolu\u00e7\u00e3o para o Exerc\u00edcio 04: from airflow import DAG import datetime as dt import csv from airflow.operators.python import PythonOperator from airflow.sensors.filesystem import FileSensor from airflow.operators.bash import BashOperator def calcular_medias(): medias = [] with open('notas.csv') as ficheiro: reader = csv.reader(ficheiro) for linha in reader: soma = 0 for coluna in linha: soma += int(coluna) medias.append(soma / 3) return medias def fazer_CSV(task_instance): medias = task_instance.xcom_pull(task_ids = 'Calcular_medias') print(medias) with open('./medias.csv', 'w') as csvfile: for media in medias: csv.writer(csvfile, delimiter=',').writerow([media]) with DAG('exerc\u00edcio4', start_date = dt.datetime(2022,8,16), schedule_interval = '0 9 * * *', catchup = False) as dag: Wait_for_file = FileSensor( task_id = \"wait_for_file\", filepath = \"/home/MEGA/Documents/Projetos pessoais/IC/airflow-test/notas.csv\", poke_interval = 5, timeout = 5 * 60, ) Calcular_medias = PythonOperator( task_id = 'Calcular_medias', python_callable = calcular_medias ) Fazer_CSV = PythonOperator( task_id = 'Fazer_CSV', python_callable = fazer_CSV ) Message = BashOperator( task_id = 'Message', bash_command = 'echo \"As m\u00e9dias foram calculadas com sucesso!\"' ) Wait_for_file >> Calcular_medias >> Fazer_CSV >> Message","title":"Resolu\u00e7\u00e3o 04"},{"location":"4%20-%20Agendamento%20e%20Sensores/Sensores/","text":"Sensores Al\u00e9m de execu\u00e7\u00f5es autom\u00e1ticas realizadas em intervalos de tempo, podemos querer disparar uma DAG sempre que um crit\u00e9rio condicional for atendido. No Airflow, podemos fazer isso atrav\u00e9s de Sensores. Um sensor \u00e9 um tipo especial de operador que verifica continuamente (em um intervalo de tempo) se uma certa condi\u00e7\u00e3o \u00e9 verdadeira ou falsa. Se verdadeira, o sensor tem seu estado alterado para bem-sucedido e o restante do pipeline \u00e9 executado. Se falsa, o sensor continua tentando at\u00e9 que a condi\u00e7\u00e3o seja verdadeira ou um tempo limite (timeout) for atingido. Um sensor muito utilizado \u00e9 o FileSensor que verifica a exist\u00eancia de um arquivo e retorna verdadeiro caso o arquivo exista. Caso contr\u00e1rio, o FileSensor retorna False e refaz a checagem ap\u00f3s um intervalo de 60 segundos (valor padr\u00e3o). Este ciclo permanece at\u00e9 que o arquivo venha a existir ou um tempo limite seja atigindo (por padr\u00e3o, 7 dias). Podemos configurar o intervalo de reavali\u00e7\u00e3o atrav\u00e9s do argumento poke_interval que espera receber, em segundos, o per\u00edodo de espera entre cada checagem. J\u00e1 o timeout \u00e9 configurado por meio do argumento timeout . from airflow.sensors.filesystem import FileSensor wait_for_file = FileSensor( task_id = \"wait_for_file\", filepath = \"/data/file.csv\", poke_interval = 10, # 10 seconds timeout = 5 * 60, # 5 minutes ) Nota: O FileSensor suporta wildcards (e.g. astericos [ * ]), o que nos permite criar padr\u00f5es de correspond\u00eancia nos nomes dos arquivos. \u26a0\ufe0f Chamamos de *poking* a rotina realizada pelo sensor de execu\u00e7\u00e3o e checagem cont\u00ednua de uma condi\u00e7\u00e3o. Condi\u00e7\u00f5es Personalizadas H\u00e1 diversos cen\u00e1rios em que as condi\u00e7\u00f5es de execu\u00e7\u00e3o de um pipeline s\u00e3o mais complexas do que a exist\u00eancia ou n\u00e3o de um arquivo. Em situa\u00e7\u00f5es como essa, podemos recorrer a uma implementa\u00e7\u00e3o baseada em Python atrav\u00e9s do PythonSensor ou ent\u00e3o criarmos nosso pr\u00f3prio sensor. O PythonSensor assim como o PythonOperator executa uma fun\u00e7\u00e3o Python. Contudo, essa fun\u00e7\u00e3o deve retornar um valor booleano: True indicando que a condi\u00e7\u00e3o foi cumprida, False caso contr\u00e1rio. J\u00e1 para criarmos nosso pr\u00f3prio sensor, basta estendermos a classe BaseSensorOperator . Informa\u00e7\u00f5es sobre a cria\u00e7\u00e3o de componentes personalizados s\u00e3o apresentados na se\u00e7\u00e3o Criando Componentes Personalizados Sensores & Deadlock O tempo limite padr\u00e3o de sete dias dos sensores possui uma falha silenciosa. Suponha uma DAG cujo schedule_interval \u00e9 de um dia. Se ao longo do tempo as condi\u00e7\u00f5es n\u00e3o foram atendidas, teremos um acumulo de DAGs e tarefas para serem executadas consider\u00e1vel. O problema deste cen\u00e1rio \u00e9 que o Airflow possui um limite m\u00e1ximo de tarefas que ele consegue executar paralalemente. Portanto, caso o limite seja atingido, as tarefas ficar\u00e3o bloqueadas e nenhuma execu\u00e7\u00e3o ser\u00e1 feita. Este comportamento \u00e9 denominado sensor deadlock . Embora seja poss\u00edvel aumentar o n\u00famero de tarefas execut\u00e1veis em paralelo, as pr\u00e1ticas recomendadas para evitar esse tipo de situa\u00e7\u00e3o s\u00e3o: Definir um timeout menor que o schedule_interval . Com isso, as execu\u00e7\u00f5es ir\u00e3o falhar antes da pr\u00f3xima come\u00e7ar. Alterar a forma como os sensores s\u00e3o acionados pelo scheduler . Por padr\u00e3o, os sensores trabalham no modo poke (o que pode gerar o deadlock ). Atrav\u00e9s do argumento mode presente na classe do sensor em quest\u00e3o, podemos alterar o acionamento do sensor para reschedule . No modo reschedule , o sensor s\u00f3 permanecer\u00e1 ativo como uma tarefa enquanto estiver fazendo as verifica\u00e7\u00f5es. Ao final da verifica\u00e7\u00e3o, caso o crit\u00e9rio de sucesso n\u00e3o seja cumprido, o scheduler colocar\u00e1 o sensor em estado de espera, liberando assim uma posi\u00e7\u00e3o ( slot ) para outras tarefas serem executadas. Observa\u00e7\u00f5es O material desta aula foi fortemente baseado neste artigo .","title":"Sensores"},{"location":"4%20-%20Agendamento%20e%20Sensores/Sensores/#sensores","text":"Al\u00e9m de execu\u00e7\u00f5es autom\u00e1ticas realizadas em intervalos de tempo, podemos querer disparar uma DAG sempre que um crit\u00e9rio condicional for atendido. No Airflow, podemos fazer isso atrav\u00e9s de Sensores. Um sensor \u00e9 um tipo especial de operador que verifica continuamente (em um intervalo de tempo) se uma certa condi\u00e7\u00e3o \u00e9 verdadeira ou falsa. Se verdadeira, o sensor tem seu estado alterado para bem-sucedido e o restante do pipeline \u00e9 executado. Se falsa, o sensor continua tentando at\u00e9 que a condi\u00e7\u00e3o seja verdadeira ou um tempo limite (timeout) for atingido. Um sensor muito utilizado \u00e9 o FileSensor que verifica a exist\u00eancia de um arquivo e retorna verdadeiro caso o arquivo exista. Caso contr\u00e1rio, o FileSensor retorna False e refaz a checagem ap\u00f3s um intervalo de 60 segundos (valor padr\u00e3o). Este ciclo permanece at\u00e9 que o arquivo venha a existir ou um tempo limite seja atigindo (por padr\u00e3o, 7 dias). Podemos configurar o intervalo de reavali\u00e7\u00e3o atrav\u00e9s do argumento poke_interval que espera receber, em segundos, o per\u00edodo de espera entre cada checagem. J\u00e1 o timeout \u00e9 configurado por meio do argumento timeout . from airflow.sensors.filesystem import FileSensor wait_for_file = FileSensor( task_id = \"wait_for_file\", filepath = \"/data/file.csv\", poke_interval = 10, # 10 seconds timeout = 5 * 60, # 5 minutes ) Nota: O FileSensor suporta wildcards (e.g. astericos [ * ]), o que nos permite criar padr\u00f5es de correspond\u00eancia nos nomes dos arquivos. \u26a0\ufe0f Chamamos de *poking* a rotina realizada pelo sensor de execu\u00e7\u00e3o e checagem cont\u00ednua de uma condi\u00e7\u00e3o.","title":"Sensores"},{"location":"4%20-%20Agendamento%20e%20Sensores/Sensores/#condicoes-personalizadas","text":"H\u00e1 diversos cen\u00e1rios em que as condi\u00e7\u00f5es de execu\u00e7\u00e3o de um pipeline s\u00e3o mais complexas do que a exist\u00eancia ou n\u00e3o de um arquivo. Em situa\u00e7\u00f5es como essa, podemos recorrer a uma implementa\u00e7\u00e3o baseada em Python atrav\u00e9s do PythonSensor ou ent\u00e3o criarmos nosso pr\u00f3prio sensor. O PythonSensor assim como o PythonOperator executa uma fun\u00e7\u00e3o Python. Contudo, essa fun\u00e7\u00e3o deve retornar um valor booleano: True indicando que a condi\u00e7\u00e3o foi cumprida, False caso contr\u00e1rio. J\u00e1 para criarmos nosso pr\u00f3prio sensor, basta estendermos a classe BaseSensorOperator . Informa\u00e7\u00f5es sobre a cria\u00e7\u00e3o de componentes personalizados s\u00e3o apresentados na se\u00e7\u00e3o Criando Componentes Personalizados","title":"Condi\u00e7\u00f5es Personalizadas"},{"location":"4%20-%20Agendamento%20e%20Sensores/Sensores/#sensores-deadlock","text":"O tempo limite padr\u00e3o de sete dias dos sensores possui uma falha silenciosa. Suponha uma DAG cujo schedule_interval \u00e9 de um dia. Se ao longo do tempo as condi\u00e7\u00f5es n\u00e3o foram atendidas, teremos um acumulo de DAGs e tarefas para serem executadas consider\u00e1vel. O problema deste cen\u00e1rio \u00e9 que o Airflow possui um limite m\u00e1ximo de tarefas que ele consegue executar paralalemente. Portanto, caso o limite seja atingido, as tarefas ficar\u00e3o bloqueadas e nenhuma execu\u00e7\u00e3o ser\u00e1 feita. Este comportamento \u00e9 denominado sensor deadlock . Embora seja poss\u00edvel aumentar o n\u00famero de tarefas execut\u00e1veis em paralelo, as pr\u00e1ticas recomendadas para evitar esse tipo de situa\u00e7\u00e3o s\u00e3o: Definir um timeout menor que o schedule_interval . Com isso, as execu\u00e7\u00f5es ir\u00e3o falhar antes da pr\u00f3xima come\u00e7ar. Alterar a forma como os sensores s\u00e3o acionados pelo scheduler . Por padr\u00e3o, os sensores trabalham no modo poke (o que pode gerar o deadlock ). Atrav\u00e9s do argumento mode presente na classe do sensor em quest\u00e3o, podemos alterar o acionamento do sensor para reschedule . No modo reschedule , o sensor s\u00f3 permanecer\u00e1 ativo como uma tarefa enquanto estiver fazendo as verifica\u00e7\u00f5es. Ao final da verifica\u00e7\u00e3o, caso o crit\u00e9rio de sucesso n\u00e3o seja cumprido, o scheduler colocar\u00e1 o sensor em estado de espera, liberando assim uma posi\u00e7\u00e3o ( slot ) para outras tarefas serem executadas.","title":"Sensores &amp; Deadlock"},{"location":"4%20-%20Agendamento%20e%20Sensores/Sensores/#observacoes","text":"O material desta aula foi fortemente baseado neste artigo .","title":"Observa\u00e7\u00f5es"}]}